[{"content":"","date":"27 June 2025","externalUrl":null,"permalink":"/notes/foundation/game101/","section":"Notes","summary":"","title":"Game101","type":"notes"},{"content":" Simple, yet powerful. Learn how to use Blowfish and its features. ","date":"27 June 2025","externalUrl":null,"permalink":"/notes/","section":"Notes","summary":"","title":"Notes","type":"notes"},{"content":"","date":"27 June 2025","externalUrl":null,"permalink":"/","section":"Runming Chen","summary":"","title":"Runming Chen","type":"page"},{"content":" Simple, yet powerful. Learn how to use Blowfish and its features. ","date":"26 June 2025","externalUrl":null,"permalink":"/notes/courses/","section":"Notes","summary":"","title":"Course","type":"notes"},{"content":" Simple, yet powerful. Learn how to use Blowfish and its features. ","date":"26 June 2025","externalUrl":null,"permalink":"/notes/courses/zhuangdongcourse/","section":"Notes","summary":"","title":"ZhuangDong's Course","type":"notes"},{"content":" 01 General Rendering Process # Rendering Pipeline # flowchart LR Model --\u003e InputStructure InputStructure --\u003e VertexShader VertexShader --\u003e OutputStructure OutputStructure --\u003e PixelShader Model # Contains vertex information (e.g., v 1.0 1.0 -1.0, ID determined by order), triangle face information (e.g., f 5 3 1, numbers are vertex IDs), as well as UV, normals, vertex colors, etc.\nInput Structure # Select the model information that needs to be used.\nVertex Shader # Process input information, convert each vertex position to screen space, and calculate/assign other per-vertex information (such as UV, vertex colors, normals, etc.).\nOutput Structure # Output specified vertex information.\nPixel Shader # Combine environment, lighting, camera, etc., to output the final rendering result.\nLighting Model # Taking the Lambert lighting model as an example, it is the result of the dot product of two vectors.\nVectors # For detailed explanation of vectors, please refer to this chapter:\nDot Product # The dot product result of the main light source vector and normal vector is in the [-1,1] range, determining the light-dark boundary.\nLambert # max(0, nDir·lDir), only takes positive values.\nHalf Lambert # Lambert * 0.5 + 0.5, makes the dark areas softer.\nCode # float lambert = max(0.0, nDotl); float2 uv = float2(lambert, 0.0); float4 var_MainTex = tex2D(_MainTex, uv); return float4(var_MainTex); 02 Case Studies # Normal Offset Creating Multiple Highlights # Using normal offset to create multiple highlight areas, enriching the highlight performance on object surfaces.\nScreen UV \u0026amp; Depth Value # By multiplying screen UV with depth value, textures can be attached to object surfaces and always face the camera.\nAlgorithm Combination # Texture Node # Multiply screen coordinates by tiling count, take the decimal part and limit it to the [-0.5, 0.5] range, then connect to the Length node.\nPower # Perform power operation on values to adjust the shape of highlights or other effects.\nLength # Length = √(x^2 + y^2 + z^2), when x=y, the result is √0.5 when x=-0.5 or 0.5, and 0 when x=0, forming a periodic cycle.\n03 Basic Code Material FlatCol # FlatCol represents a single-color rendering result. Previous cases were made using ShaderForge node method, but here we start implementing materials with code. Code-implemented materials have advantages in performance and flexibility because the final output instructions are more streamlined.\nCode Example # Shader \u0026#34;Zhuangdong/AP1/L02/Lambert_U_01\u0026#34; { Properties{} SubShader{ Tags { \u0026#34;RenderType\u0026#34; = \u0026#34;Opaque\u0026#34; } Pass { Name \u0026#34;FORWARD\u0026#34; Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardBase\u0026#34; } CGPROGRAM//Shader compilation directive #pragma vertex vert//Declare vertex shader #pragma fragment frag//Declare fragment shader #include \u0026#34;UnityCG.cginc\u0026#34;//Include Unity built-in CG library #pragma multi_compile_fwdbase_fullshadows//Enable shadow functionality #pragma target 3.0//Specify shader model version //The above are similar to context, but with more functionality. //Declare vertex shader and fragment shader, so the vert, frag functions below can be enabled. //You can also choose to enable and disable features, such as enabling shadow functionality. And specify shader model version struct VertexInput { //Import model vertex information and normal information //Words like POSITION NORMAL can be found in UNITY official Document to see their meaning float4 vertex : POSITION; float3 normal : NORMAL; }; struct VertexOutput { //Convert the above model vertex information to vertex screen position //Convert model normal information to world space normal information float4 pos : SV_POSITION; float3 nDirWS : TEXCOORD0; }; VertexOutput vert(VertexInput v) { //Fixed vertex shader writing: v2f vert (appdata v), Output vert (input variable name) VertexOutput o = (VertexOutput)0; o.pos = UnityObjectToClipPos(v.vertex);//Convert model vertex position to vertex screen position o.nDirWS = UnityObjectToWorldNormal(v.normal);//Convert normal to world normal return o; } float4 frag(VertexOutput i) : COLOR{ //Fixed fragment shader writing float3 nDir = i.nDirWS; float3 lDir = normalize(_WorldSpaceLightPos0.xyz); //0 represents directional light, xyz represents direction, 1 represents point light, xyz represents point light coordinates //normalize ensures rendering results don\u0026#39;t go wrong float nDotl = (dot(nDir, lDir)*0.5+0.5);//halflambert float lambert = max(0.0, nDotl); //Some mobile devices treat 0 as 0.1 or similar decimals, so it\u0026#39;s better to write 0.0, otherwise errors will occur. //Using max instead of clamp is because clamp limits both ends, max limits one end, more performance efficient, because dot product won\u0026#39;t be greater than 1 return float4(lambert, lambert, lambert, 1.0); } ENDCG } } FallBack \u0026#34;Diffuse\u0026#34; } Custom Material Cases: # Use float4 to customize light direction, implementing custom Lambert model Lambert model multiplied by float4 can customize light color Lambert model multiplied by float can customize light intensity 04 Case Studies # SSS Material # The core idea of this case is to use textures and parameters together to control the color and range of light-dark boundaries. The first half of the texture U-axis is black, and the second half gradually changes from dark to bright. By adjusting parameters, the color and range near the light-dark boundary line can be changed arbitrarily. This effect is commonly used to simulate the translucency of biological skin, called SSS effect.\nColor Generated Mask # This case uses the step function to layer the lighting model. The step function can turn grayscale images into black and white masks. Use custom colors, mix R/G/B channels with Lambert respectively, generate different masks, and finally mix all masks for output.\nSpecial Case # This case mixes textures and lighting models, then partitions the mixed results through Round to obtain custom results.\nUse two groups of textures with different Tiling values for mixing, then adjust grayscale to get the final texture Use Half Dir and normal dot product to get a special lighting model that can make lighting change with camera Mix grayscale image and lighting model, then process the result with round node to get only black and white boundaries Finally assign different colors to black and white areas respectively Pre-Integrated Skin Shading # LUT (Look Up Texture) principle brief description\n$$ D(\\theta) = \\frac{\\int_{-\\pi}^{\\pi} \\cos(\\theta + x) \\cdot R(2\\sin(x/2))dx}{\\int_{-\\pi}^{\\pi} R(2\\sin(x/2))dx} $$\n05 Diffuse and Specular Reflection # Diffuse Reflection # Lambert, nDotl, direction independent. Real-world examples include movie screens.\nSpecular Reflection # Phong/Blinn-Phong, rDotv/nDoth, view direction dependent. Real-world examples include car paint.\nCommon Vectors # nDir Normal direction lDir Light direction vDir View direction rDir Light reflection direction, r = reflect(-l, n) hDir Half-angle direction, vector between ldir and vdir nDoth The closer hDir and nDir are, the closer the output value is to 1 Highlight Adjustment # Power = Value^Exp is a formula commonly used in Phong model, controlling highlight range through Exp\nCase # Combining diffuse and specular reflection\n// Fragment shader example float3 nDir = normalize(i.nDirWS); float3 lDir = _WorldSpaceLightPos0.xyz; float3 vDir = normalize(_WorldSpaceCameraPos.xyz - i.posWS.xyz); float3 rDir = reflect(-lDir, nDir); float rDotv = dot(rDir, vDir); float nDotl = dot(nDir, lDir); float Lambert = nDotl * 0.5 + 0.5; float Phong = pow(max(0.0, rDotv), _SpecPow); float3 FinalColor = _BaseColor * Lambert + Phong; return float4(FinalColor, 1.0); Note: TEXCOORD is equivalent to register slots reserved for developers in the GPU pipeline, default is float4\nRegister placeholder: Marks the input position of vertex data in the shader Purpose: Pass UV and custom vector data 06 FakeEnvReflect/Grape/Painted Metal/BRDF # FakeEnvReflect # This case maps the grayscale values of cloud2 texture to U coordinates, implementing random grayscale distribution applied to lighting effects.\nGrape # The case uses Lambert lighting model and LUTRampTex as diffuse reflection, while the highlight part uses Phong model. Finally, adjust color and highlight changes through cloud2 texture.\nPainted Metal # This case uses cloud2 to generate masks, lerp mixing different material performances. One part has specular reflection, the other doesn\u0026rsquo;t.\nBRDF # Can be understood as a function that inputs light, view angle and surface parameters, outputs reflection distribution. Lambert and Phong are common BRDF models.\nBRDFExplorer # Can customize parameters, create your own BRDF, and view source code of common Shaders.\n07 3ColAmbient and Shadow # Three-Color Ambient Light # This case uses the three channels of normal to create Top/Side/Bottom three-layer masks. Used to simulate the influence of three different directions of ambient light on the lighting model.\nfloat3 nDir = i.nDirWS; float TopMask = max(0.0, nDir.g); float BotMask = max(0.0, -nDir.g); float MidMask = 1 - TopMask - BotMask; float3 EnvCol = _TopCol * TopMask + _MidCol * MidMask + _BotCol * BotMask; float AO = tex2D(_AoMap, i.uv); float3 EnvLighting = EnvCol * AO; return float4(EnvLighting, 1.0); Shadow # Add shadows to the lighting model\n//Add shadows to Shader struct VertexOutput { ... LIGHTING_COORDS(3,4)};//Add LIGHT_COORDS to VertexOutput VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; ... TRANSFER_VERTEX_TO_FRAGMENT(o)//Add TRANSFER_VERTEX_TO_FRAGMENT to vertex shader ... return o;} float4 frag(VertexOutput i) : COLOR{ float shadow = LIGHT_ATTENUATION(i);//Add LIGHT_ATTENUATION to fragment shader ...} Lighting Composition # flowchart LR Lighting --\u003e LightSource LightSource --\u003e DiffuseHalfLambert LightSource --\u003e SpecularPhong DiffuseHalfLambert --\u003e OcclusionShadow SpecularPhong --\u003e OcclusionShadow OcclusionShadow --\u003e Result Lighting --\u003e Environment Environment --\u003e Diffuse1Col Environment --\u003e SpecularCubemap Diffuse1Col --\u003e OcclusionNoAO SpecularCubemap --\u003e OcclusionNoAO OcclusionNoAO --\u003e Result RimLight --\u003e Result Emission --\u003e Result OldSchool case is the output result of the above lighting composition\n08 NormalMap Implementation Principle # TBN: Tangent(Red), Bitangent(Blue), Normal(Green)\nNormal maps record normal orientations in model space, which need to be converted to world space for normal display. Use TBN matrix to convert normal maps to world space. Conversion steps are as follows:\nSample normal map and decode Construct TBN matrix Convert tangent space normal to world space Output world space normal VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; o.pos = UnityObjectToClipPos(v.vertex); o.uv0 = v.uv0; o.posWS = mul(unity_ObjectToWorld, v.vertex); o.nDirWS = UnityObjectToWorldNormal(v.normal); o.tDirWS = normalize( mul( unity_ObjectToWorld, float4( v.tangent.xyz, 0.0 ) ).xyz );//Tangent direction OStoWS o.bDirWS = normalize(cross(o.nDirWS, o.tDirWS) * v.tangent.w);//Calculate bDir based on nDir tDir TRANSFER_VERTEX_TO_FRAGMENT(o) return o; } float4 frag(VertexOutput i) : COLOR{ //Vectors float3 nDirTS = UnpackNormal(tex2D(_NormalMap, i.uv0)).rgb; float3x3 TBN = float3x3(i.tDirWS, i.bDirWS, i.nDirWS);//TBN matrix float3 nDirWS = normalize(mul(nDirTS, TBN));//Convert to world space 09 Fresnel, Matcap, Cubemap # Fresnel # Edge glow phenomenon, formula is nDotv, outputs 1 when nDir and vDir are perpendicular, outputs 0 when coincident, visually appears as model edge highlighting\nMatcap # View space normal mapping BRDF rendering result, only suitable for static display, Zbrush preview interface uses Matcap.\nThe texture is circular because MatcapMap takes the RG channels of nDirVS as mapping values, and the normal vector satisfies the following formula:\n$$ x^2 + y^2 + z^2 = 1 $$\nWhen only taking xy, the mapped result will be a circle\nCubemap # Panoramic image, mapping the environment around the camera to a Cube. Mipmap shows different levels of clarity\nfloat3 vrDirWS = reflect(-vDirWS, nDirWS); float3 cubemap = texCUBElod(_CubeMap, float4(vrDirWS, _CubemapMip)); 10 Live Q\u0026amp;A # Modified PBR # One of the origins of PBR is the generalization of surface parameters. The more Shader functions are added, the more panel parameters there are. Finding physical relationships between parameters to reduce panel parameters is one of the contents of PBR. Modifying based on PBR\u0026rsquo;s physics-based core is called modified PBR. Transplanting some PBR BRDF or texture names to traditional models is still traditional models, not called modified PBR.\nThe significance of mobile PBS physics-based core:\nPhysics-based energy conservation Physics-based surface property generalization Microsurface theory Changing UV # Normal maps record normal information in tangent space. The main and secondary tangent directions in tangent space can be intuitively understood as the UV axis directions of the texture at the surface. When the model doesn\u0026rsquo;t change but tangent space changes, the recorded normal information will also change.\n11 Common Parameters # Common Parameters and Functions # Values, ranges _Name (\u0026ldquo;Name\u0026rdquo;, float) = defaultVal _Name (\u0026ldquo;Name\u0026rdquo;, range(min, max)) = defaultVal _Name (\u0026ldquo;Name\u0026rdquo;, int) = defaultVal Position, vector, color _Name (\u0026ldquo;Name\u0026rdquo;, vector) = (xVal, yVal, zVal, wVal) _Name (\u0026ldquo;Name\u0026rdquo;, color) = (rVal, gVal, bVal, aVal) 2D, 3D textures, environment sphere _Name (\u0026ldquo;Name\u0026rdquo;, 2d) = \u0026ldquo;defaultVal\u0026rdquo; {} _Name (\u0026ldquo;Name\u0026rdquo;, 3d) = \u0026ldquo;defaultVal\u0026rdquo; {} _Name (\u0026ldquo;Name\u0026rdquo;, cube) = \u0026ldquo;defaultVal\u0026rdquo; {} [HideInInspector] Purpose: Hide this parameter in the panel Can be used for: Any parameter [HideInInspector] _FakeLightDir(\u0026ldquo;Fake light direction\u0026rdquo;, vector) = (0.0, 1.0, 0.0, 1.0) [NoScaleOffset] Purpose: Disable texture TilingOffset panel Can be used for: Texture parameters [NoScaleOffset] _MainTex (\u0026ldquo;Main texture\u0026rdquo;, 2d) = \u0026ldquo;white\u0026rdquo; {} [Normal] Purpose: Mark this texture parameter as normal map to activate related self-check functions Can be used for: 2D texture parameters Example: [Normal] _NormTex (\u0026ldquo;Normal map\u0026rdquo;, 2d) = \u0026ldquo;bump\u0026rdquo; {} [HDR] Purpose: Used to set high dynamic range color values; such as: light colors, emission colors, etc. Can be used for: Color parameters [HDR] _EmitCol (\u0026ldquo;Emission color\u0026rdquo;, color) = (1.0, 1.0, 1.0, 1.0) [Gamma] Purpose: Used for color space conversion of color parameters; generally used for projects with Linear color space Can be used for: Color parameters Example: [Gamma] _EmitCol (\u0026ldquo;Emission color\u0026rdquo;, color) = (1.0, 1.0, 1.0, 1.0) [PowerSlider(value)] Purpose: Perform Power processing on range parameters before passing to Shader; correct some parameter adjustment feel Example: [PowerSlider(0.5)] _SpecPow (\u0026ldquo;Specular power\u0026rdquo;, range(1,90)) = 30 [Header(Label)] Purpose: Label, used for layout Can be used for: Standalone use Example: [Header(Texture)] [Space(value)] Purpose: Empty line, used for layout Can be used for: Standalone use Example: [Space(50)] Common Parameter Types # fixed: 11-bit fixed point, -2.0 ~ 2.0, precision 1/256 half: 16-bit floating point, -60000 ~ 60000, precision about 3 decimal places float: 32-bit floating point, -3.4E38 ~ 3.4E28, precision about 6, 7 decimal places int: 32-bit integer, rarely used bool: Boolean type, rarely used Matrices: float2x2, float3x3, float4x4, float2x3 and similar formats half2x2, half3x3, half4x4, half2x3 and similar formats Texture objects: sampler2D: 2D texture sampler3D: 3D texture samplerCUBE: Cube texture Parameter Usage Methods # In principle, prioritize using the lowest precision data type\nExperience # World space positions and UV coordinates, use float Vectors, HDR colors, use half; upgrade to float as needed LDR colors, simple multipliers, can use fixed Important Notes # Different platforms have different support for data types; generally automatic conversion, very rarely automatic conversion brings problems On some platforms, data type precision conversion consumption is not small; so fixed should be used carefully Discuss more with graphics developers Accessible Vertex Input Data # POSITION Vertex position float3 float4 TEXCOORD0 UV channel 1 float2 float3 float4 TEXCOORD1 UV channel 2 float2 float3 float4 TEXCOORD2 UV channel 3 float2 float3 float4 TEXCOORD3 UV channel 4 float2 float3 float4 NORMAL Normal direction float3 TANGENT Tangent direction float4 COLOR Vertex color float4 Common Vertex Output Data (more customizable than the former) # pos Vertex position CS float4 uv0 General texture UV float2 uv1 LightmapUV float2 posWS Vertex position WS float3 nDirWS Normal direction WS half3 tDirWS Tangent direction WS half3 bDirWS Bitangent direction WS half3 color Vertex color fixed4 Common Vertex Shader Operations # Note: Unity2019.3.2f1 version\npos UnityObjectToClipPos(v.vertex);\nuv0 o.uv0 = v.uv1; o.uv0 = TRANSFORM_TEX(v.uv0, _MainTex_ST);\nuv1 o.uv1 = v.uv1; o.uv1 = v.uv1 * unity_LightmapST.xy + unity_LightmapST.zw;\nposWS mul(unity_ObjectToWorld, v.vertex);\nnDirWS UnityObjectToWorldNormal(v.normal);\ntDirWS normalize(mul(unity_ObjectToWorld, float4(v.tangent.xyz, 0.0)).xyz);\nbDirWS normalize(cross(o.nDirWS, o.tDirWS) * v.tangent.w);\ncolor o.color = v.color;\nEnable texture tiling\nuniform sampler2D _MainTex; uniform float4 _MainTex_ST ... VertexOutput vert(VertexInput v) { ... o.uv0 = TRANSFORM_TEX(v.uv0, _MainTex);\u0026amp; o.uv0 = v.uv0 * _MainTex_ST.xy + _MainTex_ST.zw; ...} Function modularization, code reuse\nOriginal writing float TopMask = max(0.0, nDirWS.g); float BotMask = max(0.0, -nDirWS.g); float MidMask = 1 - TopMask - BotMask; float3 EnvCol = _TopCol * TopMask + _MidCol * MidMask + _BotCol * BotMask; Modified version float3 TriColAmbient (float3 n, float3 uCol, float3 sCol, float3 dCol){ float uMask = max(0.0, n.g); float dMask = max(0.0, -n.g); float sMask = 1.0 - uMask - dMask; float3 envCol = uCol * uMask + sCol * sMask + dCol * dMask; return envCol; } ... float3 envCol = TriColAmbient(nDirWS, _EnvUpCol, _EnvSideCol, _EnvDownCol); ... Modularization Create Assets\\Cgnic\\MyCgnic.cgnic\n#ifndef MY_CGINC #define MY_CGINC float3 TriColAmbient (float3 n, float3 uCol, float3 sCol, float3 dCol){ float uMask = max(0.0, n.g); float dMask = max(0.0, -n.g); float sMask = 1.0 - uMask - dMask; float3 envCol = uCol * uMask + sCol * sMask + dCol * dMask; return envCol;} #endif Add path under Shader CGPROGRAM\n... CGPROGRAM ... #include \u0026#34;../cginc/MyCginc.cginc\u0026#34; ... float3 envCol = TriColAmbient(nDirWS, _EnvUpCol, _EnvSideCol, _EnvDownCol); 12 Ogre Case # This case references https://support.steampowered.com/kb/3081-QUXN-6209/dota-2-workshop-item-shader-masks?l=finnish , combined with actual resources to create a typical character Shader.\nCode Example # Shader \u0026#34;Zhuangdong/AP1/L06/L06_ogre_Feedback\u0026#34; { Properties{ [Header(Texture)] _MainTex (\u0026#34;RGB:Base color A:Transparency texture\u0026#34;, 2D) = \u0026#34;White\u0026#34; {} [Normal] _NormalMap (\u0026#34;RGB:Normal map\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _DetailMap (\u0026#34;RGB:Detail texture A:Detail mask\u0026#34;, 2D) = \u0026#34;black\u0026#34; {} _MetalnessMask (\u0026#34;Metal mask\u0026#34;, 2D) = \u0026#34;black\u0026#34; {} _SelfIllMask (\u0026#34;SelfIll emission mask\u0026#34;, 2D) = \u0026#34;black\u0026#34; {} _SpecTex (\u0026#34;RGB:Spec highlight texture\u0026#34;, 2D) = \u0026#34;gray\u0026#34; {} _RimLight (\u0026#34;Rim edge light mask\u0026#34;, 2D) = \u0026#34;black\u0026#34; {} _BaseTintMask (\u0026#34;Tint base color mask\u0026#34;, 2D) = \u0026#34;White\u0026#34; {} _SpecularExponent (\u0026#34;SpecExpo specular reflection exponent\u0026#34;, 2D) = \u0026#34;White\u0026#34; {} _DiffuseWarp (\u0026#34;Diffuse diffusion mask\u0026#34;, 2D) = \u0026#34;black\u0026#34; {} _CubeMap (\u0026#34;RGB:Cube environment texture\u0026#34;, Cube) = \u0026#34;_Skybox\u0026#34; {} _FresnelWarp(\u0026#34;Fresnel texture R:FCol G:FRim B:FSpec\u0026#34;, 2D) = \u0026#34;black\u0026#34; {} [Header(Diffuse)] _LightCol (\u0026#34;Main light color\u0026#34;, Color) = (1.0,1.0,1.0,1.0) [Space(10)] _TopCol (\u0026#34;Top color\u0026#34;, Color) = (0.47,0.96,1,1) _MidCol (\u0026#34;Middle color\u0026#34;, Color) = (0.46,0.7,0.45,1) _BotCol (\u0026#34;Bottom color\u0026#34;, Color) = (0.75,0.39,0.39,1) _EnvDiffInt (\u0026#34;EnvDiff ambient diffuse intensity\u0026#34;, Range(0.0, 5.0)) = 0.2 [Header(Specular)] _SpecPow(\u0026#34;Highlight power\u0026#34;, Range(0.0, 90.0)) = 5 _SpecInt(\u0026#34;Highlight intensity\u0026#34;, Range(0.0, 10.0)) = 5 [Space(10)] _EnvSpecint (\u0026#34;Envspec ambient specular intensity\u0026#34;, Range(0.0, 10.0)) = 0.2 [Header(SelfIll)] _SelfIllInt (\u0026#34;SelfIll emission intensity\u0026#34;, Range(0, 10)) = 1 [HDR]_RimCol (\u0026#34;Rim light color\u0026#34;, Color) = (1.0,1.0,1.0,1.0) _RimInt (\u0026#34;Rim light intensity\u0026#34;, Range(0.0, 3.0)) = 1.0 [HideInInspector] _Cutoff (\u0026#34;Alpha cutoff\u0026#34;, Range(0,1)) = 0.5 [HideInInspector] _Color (\u0026#34;Main Color\u0026#34;, Color) = (1,1,1,1)//Fallback Require } SubShader{ Tags { \u0026#34;RenderType\u0026#34; = \u0026#34;Opaque\u0026#34; } Pass { Name \u0026#34;FORWARD\u0026#34; Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardBase\u0026#34; } Cull Off CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34;//Automatically handle lighting attenuation for shadow processing #include \u0026#34;Lighting.cginc\u0026#34;//Mainly used to access ambient main directional light related data #pragma multi_compile_fwdbase_fullshadows //Enable shadow functionality #pragma target 3.0 //Texture uniform sampler2D _MainTex; uniform sampler2D _NormalMap; uniform sampler2D _DetailMap; uniform sampler2D _MetalnessMask; uniform sampler2D _SelfIllMask; uniform sampler2D _SpecTex; uniform sampler2D _RimLight; uniform sampler2D _BaseTintMask; uniform sampler2D _SpecularExponent; uniform sampler2D _DiffuseWarp; uniform samplerCUBE _CubeMap; uniform sampler2D _FresnelWarp; //Diffuse uniform half3 _LightCol; uniform fixed3 _TopCol; uniform fixed3 _MidCol; uniform fixed3 _BotCol; uniform fixed _EnvDiffInt; //Specular uniform half _SpecPow; uniform half _SpecInt; uniform half _EnvSpecint; //SelfIll uniform fixed _SelfIllInt; uniform half3 _RimCol; uniform half _RimInt; uniform half _Cutoff; struct VertexInput { //Check Document for meaning float4 vertex : POSITION; float2 uv0 : TEXCOORD0; float3 normal : NORMAL; float4 tangent : TANGENT; }; struct VertexOutput { //Normal information float4 pos : SV_POSITION; float2 uv0 : TEXCOORD0; float3 posWS : TEXCOORD1; float3 nDirWS : TEXCOORD2; float3 tDirWS : TEXCOORD3; float3 bDirWS : TEXCOORD4; LIGHTING_COORDS(5,6) }; VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; o.pos = UnityObjectToClipPos(v.vertex); o.uv0 = v.uv0; o.posWS = mul(unity_ObjectToWorld, v.vertex); o.nDirWS = UnityObjectToWorldNormal(v.normal); o.tDirWS = normalize( mul( unity_ObjectToWorld, float4( v.tangent.xyz, 0.0 ) ).xyz ); o.bDirWS = normalize(cross(o.nDirWS, o.tDirWS) * v.tangent.w); TRANSFER_VERTEX_TO_FRAGMENT(o) return o; } float4 frag(VertexOutput i) : COLOR{ //Vectors half3 nDirTS = UnpackNormal(tex2D(_NormalMap, i.uv0)).rgb; half3x3 TBN = float3x3(i.tDirWS, i.bDirWS, i.nDirWS); half3 nDirWS = normalize(mul(nDirTS, TBN)); half3 vDirWS = normalize(_WorldSpaceCameraPos.xyz - i.posWS.xyz); half3 vrDirWS = reflect(-vDirWS, nDirWS); half3 lDirWS = _WorldSpaceLightPos0.xyz; //Represents point light coordinates, normalization is safer, dot product results will be more correct half3 lrDirWS = reflect(-lDirWS, nDirWS); float shadow = LIGHT_ATTENUATION(i); //Dot products half rDotv = dot(vDirWS, lrDirWS);//Phong half nDotl = dot(nDirWS, lDirWS);//Lambert half ndotv = dot(nDirWS, vDirWS); //Sample textures half4 var_MainTex = tex2D(_MainTex, i.uv0); half4 var_DetailMap = tex2D(_DetailMap, i.uv0); half var_MetalnessMask = tex2D(_MetalnessMask, i.uv0); half var_SelfIllMask = tex2D(_SelfIllMask, i.uv0); half var_SpecTex = tex2D(_SpecTex, i.uv0);//specInt half var_RimLight = tex2D(_RimLight, i.uv0); half var_BaseTintMask = tex2D(_BaseTintMask, i.uv0); half var_SpecularExponent = tex2D(_SpecularExponent, i.uv0);//specSize half3 var_Cubemap = texCUBElod(_CubeMap, float4(vrDirWS, lerp(8.0, 0.0, var_MetalnessMask))).rgb; half3 var_FresnelWarp = tex2D(_FresnelWarp, ndotv); //Extract information half3 BaseCol = var_MainTex.rgb; half Opacity = var_MainTex.a; half MetalMask = var_MetalnessMask; half RimLightInt = var_RimLight; half TintMask = var_BaseTintMask; half SpecExp = var_SpecularExponent; half3 EnvCube = var_Cubemap; half SpecInt = var_SpecTex; half EmitInt = var_SelfIllMask; //Lighting model half3 DiffCol = lerp(BaseCol, half3(0.0,0.0,0.0), MetalMask); //The closer to metal, the weaker the diffuse reflection half3 SpecCol = lerp(BaseCol, half3(0.3,0.3,0.3), TintMask) * SpecInt; //Determine highlight color based on TintMask //0.3 is an empirical value, this value multiplied by highlight intensity specInt gives a comfortable highlight color texture //Fresnel half3 Fresnel = lerp(var_FresnelWarp, 0.0, MetalMask); //The higher the metalness, the less obvious the Fresnel phenomenon half FreCol = Fresnel.r; //No practical use half FreRim = Fresnel.g; //Use Fresnel for rim light half FreSpec = Fresnel.b; //Use Fresnel for specular reflection //Main light diffuse reflection half HalfLambert = nDotl * 0.5 + 0.5;//Halflambert half3 var_DiffuseWarp = tex2D(_DiffuseWarp, half2(HalfLambert, 0.2)); //Sample Ramptexture half3 DirDiff = DiffCol * var_DiffuseWarp * _LightCol; //Main light specular reflection half Phong = pow(max(0.0, rDotv), SpecExp * _SpecPow); half Spec = Phong * max(0.0,nDotl); Spec = max(Spec, FreSpec); //Will have a glossy visual effect, strong Fresnel phenomenon mixed with Phong Spec = Spec * _SpecInt; //After multiplying by SpecInt, most of the previous Spec effects will disappear, only the range specified by SpecInt will have highlights //The original Shader integrates all specular reflections together and finally does max(flMetalnessMask, flSpecWarp) //Here the author splits specular reflections, so both sides need to do a max at the final calculation half3 DirSpec = SpecCol * Spec * _LightCol; //Ambient diffuse reflection float TopMask = max(0.0, nDirWS.g); float BotMask = max(0.0, -nDirWS.g); float MidMask = 1 - TopMask - BotMask; float3 EnvCol = _TopCol * TopMask + _MidCol * MidMask + _BotCol * BotMask; float3 EnvDiff = DiffCol * EnvCol * _EnvDiffInt; //The video author only used single color, here I directly copied the previous case code without modification //Ambient specular reflection half ReflectInt = max(FreSpec, MetalMask) * SpecInt; //Metal parts have maximum MetalMask, non-metal parts have maximum FreSpec //This way non-metal has strong Fresnel phenomenon, metal parts have little, instead have strong reflection phenomenon half3 EnvSpec = SpecCol * ReflectInt * EnvCube * _EnvSpecint; //Rim light half3 RimLight = _RimCol * FreRim * RimLightInt * max(0.0, nDirWS.g) * _RimInt; //Rim light only appears on top, so need to use normal.g //FreRim defines Fresnel phenomenon, RimLightInt defines intensity range, RimInt defines intensity //Emission float3 emission = EmitInt * DiffCol * _SelfIllInt; //Final half3 FinalRGB = (DirDiff + DirSpec) * shadow + EnvDiff + EnvSpec + RimLight + emission; clip(Opacity - _Cutoff);//Delete all less than _Cutoff, keep those greater than return float4(FinalRGB, 1.0); } ENDCG } } Fallback \u0026#34;Legacy Shaders/Transparent/Cutout/VertexLit\u0026#34; //Note: Using FallBack requires declaring a _Color: color, even if you don\u0026#39;t use it } Main Issues # Fallback setting: Specify to a Shader that supports transparency textures (such as Legacy Shaders/Transparent/Cutout/VertexLit). Cull Off: Turn off backface culling to prevent model back faces from being clipped. Texture Explanation # Non-shared textures: # Color MetalnessMask Normal RimMask SelfIllumMask SpecularExponent SpecularMask TintByBaseMask Translucency Shared textures # Cubemap DiffuseWarp RampTex FresnelWarpColor FresnelWarpRim FresnelWarpSpec Texture merging # Color: RGB: Color A: Opacity Mask1: R: SpecInt G: RimInt B: TintMask A: SpecPow Mask2: R:FresnelCol G: FresnelRim B: FresnelSpec Texture Description # MetalnessMask: Metal mask, controls metal areas SpecularMask: Highlight intensity mask RimlightMask: Edge light mask BaseTintMask: Highlight tinting mask, metal highlight color determined by ColorMap FresnelTex: Three RampTex, mapped to Fresnel lighting model Other textures such as ColorMap, Transparency, NormalMap, SelfIlluminationMask, SpecularExponent, DiffuseMask, DetailMask have been encountered in previous cases, or are not used in this case, not detailed here Source Code Explanation # RimLightScale, SpecScale: Fixed values, different for different characters, mainly used for rim light and highlight calculations.\nSpecularScale, RimLightScale and other parameters that exist in source code, the author removed them without affecting the final effect and simplified the code.\nFresnel and rim light: Different characters have different RimLightScale settings, mostly numbers greater than 1. Based on this, through FresnelWarp texture and RimMask to control the intensity and range of edge highlights, exaggerated cartoon edge effects can be achieved.\nFresnelWarpSpec is called flSpecWarp in the original Shader, used to define the character\u0026rsquo;s ambient specular reflection.\nHighlight and metal control: Metal parts use cubemap reflection, non-metal parts use highlight texture control. Through cSpecular *= max(flMetalnessMask, flSpecWarp); to achieve highlight switching between metal and non-metal.\nMain light specular reflection: Dota2 uses an algorithm more suitable for top-down view, core code as follows:\nvec3 R = reflect (V, N); float RdotL = clamp(dot(L, -R), 0, 1); From this we can know that the closer R and L are, the brighter the light. Compared to traditional Phong, it\u0026rsquo;s more suitable for Dota2\u0026rsquo;s god\u0026rsquo;s-eye view.\nLighting Composition # flowchart LR Lighting --\u003e LightSource LightSource --\u003e DiffuseHalfLambert LightSource --\u003e SpecularPhong DiffuseHalfLambert --\u003e OcclusionShadow SpecularPhong --\u003e OcclusionShadow OcclusionShadow --\u003e Result Lighting --\u003e Environment Environment --\u003e Diffuse1Col Environment --\u003e SpecularCubemap Diffuse1Col --\u003e OcclusionNoAO SpecularCubemap --\u003e OcclusionNoAO OcclusionNoAO --\u003e Result RimLight --\u003e Result Emission --\u003e Result Experience Summary # The video case splits the original code and integrates it into its own system, making it easier to understand and extend. Beginners tend to ignore the role of Fresnel textures, it\u0026rsquo;s recommended to refer more to Dota2 original Shader and\n","date":"26 June 2025","externalUrl":null,"permalink":"/notes/courses/zhuangdongcourse/forwardrendering/","section":"Notes","summary":"","title":"Zhuangdong's Technical Art Introduction Course - Forward Rendering","type":"notes"},{"content":" 01-What exactly is a vector? # Definition # A vector is a quantity that has both magnitude and direction\nRepresentation # Usually represented by arrows or directed line segments Key properties are length (magnitude) and direction Example: Vector (-2, 3) represents an arrow from origin (0,0) pointing to point (-2,3) Vector magnitude calculation formula # $$ |\\vec{v}| = \\sqrt{x^2 + y^2} $$\nCharacteristics # Multi-dimensional vectors exist (2D/3D most commonly used in gaming) Essentially an ordered list of numbers that can represent various things Vector addition # Can be viewed as moving from the origin along two vector directions, for example vector V + vector W means we first move in the direction of vector V, then move in the direction of vector W, with the movement distance being their magnitudes\nVector addition formula # $$ \\vec{a} + \\vec{b} = \\begin{bmatrix} x_1 \\ y_1 \\end{bmatrix} + \\begin{bmatrix} x_2 \\ y_2 \\end{bmatrix} = \\begin{bmatrix} x_1 + x_2 \\ y_1 + y_2 \\end{bmatrix} $$\nVector and scalar multiplication # Also called vector scaling\nScalar multiplication formula # $$ \\vec{v} = k \\cdot \\vec{v} = (k v_x, k v_y, k v_z) $$\n02-Linear combinations, span, and basis # Basis vectors # In a 2D coordinate system (xy plane), there are two special vectors: one pointing to the positive right with length 1, called \u0026ldquo;i-hat\u0026rdquo;, and another pointing straight up with length 1, called \u0026ldquo;j-hat\u0026rdquo;. These two vectors together form the \u0026ldquo;basis vectors\u0026rdquo; of this coordinate system, they are the foundational framework of the coordinate system.\nFor example, vector (3, -2) can be understood as:\nStretching the basis vector i-hat by 3 times Stretching the basis vector j-hat by 2 times The final vector (3, -2) is the sum of these two scaled vectors (3i + (-2j)) Here, the coordinate values 3 and -2 are treated as scalars, while the basis vectors are the objects being scaled by these scalars\nLinear combination # The sum of two vector scalar multiplications is called a linear combination\n$$ \\vec{U} = a\\vec{V} + b\\vec{W} $$\nwhere a,b are scalars\nSpan # Imagine you have several arrows, the span is the set of all possible positions you can reach using only these arrows through the two most fundamental operations: vector addition and scalar multiplication\nOne-dimensional # Collinear means the vectors that constitute the space are on the same line\nTwo-dimensional # Three-dimensional # There exists a vector that is not in the same plane as other vectors. When you scale this vector, it moves the plane along its direction, thus reaching any point in 3D space\nLinear dependence # If removing one vector doesn\u0026rsquo;t make the span smaller, then this vector is redundant. It itself is obtained from other vectors through scalar multiplication and addition. Such vectors are called linearly dependent\nLinear independence # If removing one vector makes the span smaller, it means this arrow is not redundant. Such vectors are called linearly independent\nBasis # A basis of a vector space is a minimal set of linearly independent vectors\nCan span the entire space Are all linearly independent ","date":"10 June 2025","externalUrl":null,"permalink":"/notes/foundation/linearalgebra/essenceoflinearalgebra_part1/","section":"Notes","summary":"","title":"EssenceOfLinearAlgebra - 01","type":"notes"},{"content":" Simple, yet powerful. Learn how to use Blowfish and its features. ","date":"10 June 2025","externalUrl":null,"permalink":"/notes/foundation/linearalgebra/","section":"Notes","summary":"","title":"LinearAlgebra","type":"notes"},{"content":"","date":"10 June 2025","externalUrl":null,"permalink":"/tags/space/","section":"Tags","summary":"","title":"Space","type":"tags"},{"content":"","date":"10 June 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Simple, yet powerful. Learn how to use Blowfish and its features. ","date":"9 June 2025","externalUrl":null,"permalink":"/notes/foundation/","section":"Notes","summary":"","title":"Foundation","type":"notes"},{"content":" Simple, yet powerful. Learn how to use Blowfish and its features. ","date":"9 June 2025","externalUrl":null,"permalink":"/portfolio/","section":"Portfolio","summary":"","title":"Portfolio","type":"portfolio"},{"content":" 22. Unity Built-in LightingMap Baking System # Pre-baking Setup # Setting up Skybox # Path: Windows → Rendering → Lighting Settings → Skybox Material Setting Scene Objects as Static # Enable Contribute GI and Reflection Probe Static Static option descriptions: Contribute GI: Responds to global illumination Occluder/Occludee Static: Responds to occlusion culling (OccCulling) Batching Static: Batching optimization Navigation Static/Off Mesh Link Generation: Navigation related Reflection Probe Static: Determines whether objects appear in reflection probe records Main Directional Light, Emission Material, Reflection Probe Settings # Set emission material Global Illumination to Baked, Color to HDR LightingMode Settings # Path: Windows → Rendering → Lighting Settings → MixedLighting → LightingMode\nLightingMode options:\nBaked Indirect Subtractive ShadowMask Directional Light Mode: RealTime Mixed Baked Note: ShadowMask Mode needs to be enabled in ProjectSetting → Quality → Shadow → Shadowmask Mode. ShadowMask Mode additionally generates a Shadowmask texture.\nCombination List # Bake Indirect Runtime LM = GI = EmitLighting + SkyLighting Shadows: Real-time shadows Mixed LM = GI = EmitLighting + SkyLighting + LightsGI Shadows: Real-time shadows Baked LM = GI + DL = (EmitLighting + SkyLighting + LightsGI) + LightsLighting Shadows: Static objects: shadows written to LM, Dynamic objects: none Subtractive Runtime LM = GI = EmitLighting + SkyLighting Shadows: Real-time shadows, RealTime Shadow Color setting disabled Mixed LM = GI + DL = (EmitLighting + SkyLighting + LightsGI) + LightsLighting Shadows: Static objects: shadows written to LM, Dynamic objects: real-time; RealTime Shadow Color setting effective Baked LM = GI + DL = (EmitLighting + SkyLighting + LightsGI) + LightsLighting Shadows: Static objects: shadows written to LM, Dynamic objects: none; RealTime Shadow Color setting disabled ShadowMask Runtime LM-light = GI = EmitLighting + SkyLighting LM-shadowmask = null Shadows: Real-time shadows Mixed LM-light = GI = EmitLighting + SkyLighting + LightsGI LM-shadowmask = LightsShadow Shadows: Static objects: shadows written to LM, Dynamic objects: real-time shadows Baked LM = GI + DL = (EmitLighting + SkyLighting + LightsGI) + LightsLighting LM-shadowmask = null Shadows: Static objects: shadows written to LM, Dynamic objects: none Lighting Explanation # EmitLighting represents lighting caused by emission materials SkyLighting represents lighting caused by skybox LightsGI represents bounce light caused by main light LightsLighting represents lighting caused by main light LM = Lightmap, DL = DirectLighting Note: Subtractive and ShadowMask handle real-time shadows for dynamic objects differently, ShadowMask provides more natural shadow mixing for static and dynamic objects.\nLightmapping Settings # Lightmapper: Progressive CPU Lightmap Resolution: 40 Directional Mode: Non-Directional (generally not recommended, effect not obvious, energy consumption doubles) Common Solutions # Full real-time lighting: No Lightmap needed Full real-time direct lighting + Baked Indirect Static objects baked, dynamic objects real-time (Subtractive/ShadowMask) Auxiliary lights only for baking (Baked/Mixed) Mixed determines whether to affect dynamic objects Effects RealTime Batching Strategy and Baking Process Analysis # Definition # Combining several batches of rendering with the same rendering process to save resources and increase speed. Generally, shared Material with same parameters can be batched.\nCommon Strategies # Unity static settings Unity SRPBatching (same Shader can be batched) GPU Instancing Manual batching (merge models in DDC software) FrameDebugger # Windows → Analysis → FrameDebugger\nFrameDebugger can view batching and rendering process\n23. External LightingMap Baking and Custom Shader # External Baking Process # Use 3dsMax/MAYA to bake LightingMap, need to create UV2 for models Textures: AO Map replaces SkyLighting EmitLighting LightsShadow LightGI + LightsLighting Alpha Use SD\u0026rsquo;s Blur HQ node to remove noise, merge textures to save resources Lighting Composition # flowchart LR Lighting --\u003e SimpleLightSource SimpleLightSource --\u003e MainDirectionalLight MainDirectionalLight --\u003e DiffuseReflection MainDirectionalLight --\u003e SpecularReflection DiffuseReflection --\u003e Occlusion SpecularReflection --\u003e Occlusion Occlusion --\u003e Result Lighting --\u003e ComplexEnvironment ComplexEnvironment --\u003e SkyLight ComplexEnvironment --\u003e Emission ComplexEnvironment --\u003e OtherAmbientLight SkyLight --\u003e DiffuseReflectionCubemap Emission --\u003e DiffuseReflection1Col OtherAmbientLight --\u003e DiffuseReflection DiffuseReflectionCubemap --\u003e Occlusion DiffuseReflection1Col --\u003e Occlusion DiffuseReflection --\u003e Ignore SkyLight --\u003e SpecularReflectionCubemap Emission --\u003e SpecularReflectionCubemap OtherAmbientLight --\u003e SpecularReflectionCubemap SpecularReflectionCubemap --\u003e SurfaceOcclusion SpecularReflectionCubemap --\u003e FresnelAttenuation SurfaceOcclusion --\u003e Result FresnelAttenuation --\u003e Result Occlusion --\u003e Result Shader Core Structure # Shader \u0026#34;Zhuangdong/AP1/L15/L15_LightingMap\u0026#34; { Properties{ // ... Texture and parameter declarations ... } SubShader{ Tags { \u0026#34;RenderType\u0026#34; = \u0026#34;Opaque\u0026#34; } Pass { Name \u0026#34;FORWARD\u0026#34; Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardBase\u0026#34; } CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; // ... Using orge case as template, vertex shader removes TRANSFER_VERTEX_TO_FRAGMENT(o) ... VertexOutput vert(VertexInput v) { ... //Case needs 2 UVs, here merge 2 UVs into one float4, saving TEXCOORD o.uvs = float4(v.uv0, v.uv1); ... } float3 DecodeNormal (float2 maskXY) { //Texture only has RG two channels, we need to supplement B channel float2 nDirTSxy = maskXY * 2.0 - 1.0; //Texture value range is (0,1), normal is vector, vector value range is (-1,1) float nDirTSz = sqrt(1.0 - (nDirTSxy.x * nDirTSxy.x + nDirTSxy.y * nDirTSxy.y)); //sqrt takes square root, x squared + y squared + z squared = 1 return float3(nDirTSxy, nDirTSz); } float4 frag(VertexOutput i) : COLOR { // ... Sampling, vectors, dot products ... //Extract surface information float occlusion = var_MaskTex.r; float matMask = var_MaskTex.g;//Metal texture float3 diffCol = var_MainTex.rgb * lerp(1.0, _MetalDarken, pow(matMask, 5.0)); //Here add _MetalDarken to metal, reducing metal part diff float specPow = max(1.0, lerp(_SpecParams.x, _SpecParams.z, matMask)); float specInt = max(0.0, lerp(_SpecParams.y, _SpecParams.w, matMask)); float reflectMip = clamp(lerp(_EnvReflectParams.x, _EnvReflectParams.z, matMask), 0.0, 7.0); //clamp is clamp(value,min,max) which limits value to (min,max) float reflectInt = max(0.0, lerp(_EnvReflectParams.y, _EnvReflectParams.w, matMask)); float fresnel = lerp(pow(1.0 - max(0.0, ndotv), _FresnelPow), 1.0, matMask); //Distinguish Fresnel intensity between metal and non-metal //Extract lighting information float skyLightOcc = var_LightMap.r; float emitLightingInt = var_LightMap.g; float mainLightGIInt = pow(var_LightMap.b, _GIpow); float mainLightShadow = var_LightMap.a; //Sample texture Cube float3 var_SkyCube = texCUBElod(_SkyCube, float4(vrDirWS, 7.0)).rgb; float3 var_EnvCube = texCUBElod(_EnvCube, float4(vrDirWS, reflectMip)).rgb; //MainLight //Diff float3 halfShadowCol = lerp(_HalfShadowCol.rgb, _MainLightCol, mainLightShadow); //First assign colors to shadow and main light float3 mainLightCol = lerp(_MainLightCol, halfShadowCol, _HalfShadowCol.a) * mainLightShadow; //Then use shadow mask to remove shadow color, leaving main light color and penumbra color, here _HalfShadowCol.a controls penumbra intensity float3 mainLightDiff = diffCol * mainLightCol * max(0.0, nDotl); //DiffCol*Lambert*MainLight //Spec float3 mainLightSpec = mainLightCol * pow(max(0.0, vDotr), specPow) * specInt;//Phong //GI float3 mainLightGI = _GICol * occlusion * mainLightGIInt * _GIInt; //GI is the bounce light of main light, here add AO for variation //Mixed float3 mainLight = (mainLightDiff + mainLightSpec + mainLightGI * _MainLightGIOn) * _MainLightOn; //Here GI already has ambient AO information, so no need to *LM //OtherLight float3 skyLightDiff = diffCol * var_SkyCube * _SkyCol * _SkyLightInt * skyLightOcc * occlusion; //Here is object diffuse * sky diffuse * LM(LightingMap) float3 emitLightDiff = diffCol * _EmissionCol * emitLightingInt * occlusion; //Same as above, this part is not affected by ambient AO, so no need to *_SkyLightOcc //OtherEnvSpec float3 envLightSpec = var_EnvCube * reflectInt * fresnel * occlusion; float3 OtherLight = skyLightDiff * _SkyLightOn + emitLightDiff * _EmitLightOn + envLightSpec * _EnvReflectOn; float3 finalRGB = mainLight + OtherLight; return float4(finalRGB, 1.0); } ENDCG } } FallBack \u0026#34;Diffuse\u0026#34; } Key Parameter Explanation # _SpecParams\nx: Non-metal specular power y: Non-metal specular intensity z: Metal specular power w: Metal specular intensity _EnvReflectParams\nx: Non-metal Cube sampling Mip value y: Non-metal reflection intensity z: Metal Cube sampling Mip value w: Metal reflection intensity Key Points Summary # Normal map only has RG, need to manually supplement B channel Penumbra coloring Metal/non-metal parameters separately control specular and reflection Synthesize lighting according to lighting composition 24-26 LightingController Custom Baker and Multi-Script Collaboration # Due to the extensive code content in this case, the explanation focuses on summaries. For source code, please download from the tutorial location.\nThis case implements a custom baker for completing custom baking, result preview, and material editing functions. The baking process generates three textures corresponding to main light, sky light, and emission light sources, and finally merges these three textures to output the final LightingMap. After baking is complete, you can preview the independent effects of each texture separately and flexibly adjust the final image performance through global parameters.\nMain Light\nSky Light\nEmission Light\nFunctions and File Structure # LightingController: Main script responsible for the entire baking process. LightingControllerGUI: Custom baking panel (UI), making operations more intuitive. EmissionShaderGUI: Allows material spheres to set emission participation in GI in the Inspector interface. Building/EmitLight/Sky: Respectively correspond to building, emission, and skybox Shaders. Global Parameter Brief Explanation # MetalDarken: Darkness of metal parts (affects metal surface reflection intensity). MainLightCol: Main light color. SpecParams: Specular parameters (x/z are specular power, y/w are specular intensity, respectively corresponding to metal/non-metal). SkyLightInt: Sky light intensity. ReflectParams: Reflection parameters (x/z are CubeMap Mip, y/w are reflection intensity). FresnelPow: Fresnel phenomenon intensity. EmissionCol: Emission color. Baking Process # Collect scene baking information: including texture count, paths, texture objects Create cache: Prepare cache for main light, sky light, emission GI and final synthesized Lightmap respectively. Bake: Bake main light, sky light, emission GI in sequence, store each step result in cache. Synthesize Lightmap: Combine the results of three baking steps into one final Lightmap. Replace old Lightmap: Overwrite the scene\u0026rsquo;s original lighting textures with the newly synthesized Lightmap. Reset scene lighting environment: Modify scene settings. Update global parameters: Synchronize parameters to all related materials. Bake reflection probes: Make reflection effects display correctly. Core Code Example # public void MultiBake() { var buffer = new LightmapsBuffer(); Bake(BakeMode.BakeMainLight); var info = new LightmapsInfo(LightmapSettings.lightmaps); buffer.WriteBuffer(info, LightmapsBuffer.BufferType.MainLight); Bake(BakeMode.BakeSkyLight); buffer.WriteBuffer(info, LightmapsBuffer.BufferType.SkyLight); Bake(BakeMode.BakeEmissionGI); buffer.WriteBuffer(info, LightmapsBuffer.BufferType.EmissionGI); buffer.CreateLightmaps(); buffer.OverrideLightmaps(info); buffer.Clear(); ArrangeBakeScene(BakeMode.Default); UpdateGlobalProperties(); BakeReflectProbe(); } LightingController # Class and Inheritance, Methods and Lifecycle # public class LightingController : MonoBehaviour class: Can be understood as a \u0026ldquo;blueprint\u0026rdquo;, like Shader is a blueprint, Material is an \u0026ldquo;item\u0026rdquo; made according to the blueprint. Inheritance: Like \u0026ldquo;vehicle\u0026rdquo; is the parent class, \u0026ldquo;car\u0026rdquo; is the child class, child class automatically has the basic functions of parent class. MonoBehaviour: All Unity scripts that can be attached to objects must inherit from it to participate in the game lifecycle. [ExecuteInEditMode]: Generally, scripts only run in PlayMode, this directive makes the script also run in preview window private void OnEnable() {UpdateGlobalProperties();}; void is different from methods in previous cases, void functions don\u0026rsquo;t return values, they are mainly used to perform specific operations. OnEnable means this function starts running at the OnEnable stage of the game lifecycle UpdateGlobalProperties() method is written after OnEnable, which is allowed in C#\nLightmapsInfo # To understand the following concepts to understand the role of LightmapsInfo\nLightmapData, struct, constructor, Dictionary and foreach loop\nLightmapData # LightmapData[] is Unity\u0026rsquo;s data type, it\u0026rsquo;s the collection of all lighting textures in the current scene lightmapsData is a custom name, representing LightmapData[], used for function internal operations LightmapsData[] contains multiple instances, each instance represents each Lightingmap, each instance contains: lightmapColor lightmapDir shadowMask occlusionMask Struct and Constructor # Struct # Can be understood as a \u0026ldquo;container\u0026rdquo; that can hold different types of data.\nSyntax # public/private name (same as class/struct name) (type, parameter name)\nConstructor # A special function used to initialize struct (or class), with the same name as the struct.\nExample # public struct LightmapsInfo { public readonly Dictionary\u0026lt;string, Texture2D\u0026gt; lightmapsInfo; public LightmapsInfo(LightmapData[] lightmapsData) { // Here all content will be automatically initialized } } public class Person { public string Name; public int Age; // Constructor public Person(string name, int age) { Name = name; Age = age; } } Without constructor, you need to manually assign values one by one, very tedious. With constructor, just new LightmapsInfo(parameters), all content automatically arranged. Multiple Uses of Constructor # Constructor can have multiple, with different parameters or methods. Can also do parameter checking in constructor to prevent errors. Example # public LightmapsInfo(LightmapData[] lightmapsData){ if(lightmapsData == null) { // Do some error handling } // Other initialization content } Dictionary # Dictionary is a \u0026ldquo;key-value pair\u0026rdquo; container that can quickly find corresponding content through \u0026ldquo;names\u0026rdquo;. For example: Dictionary\u0026lt;string, Texture2D\u0026gt;, can use string names to find corresponding texture paths and objects. Case # Dictionary\u0026lt;string, Texture2D\u0026gt; myDict = new Dictionary\u0026lt;string, Texture2D\u0026gt;(); myDict[\u0026#34;MainLight\u0026#34;] = mainLightTex; Texture2D tex = myDict[\u0026#34;MainLight\u0026#34;]; foreach Loop # foreach is used to traverse collections (such as arrays, List, Dictionary, etc.)\nExample # //data represents instance foreach (var data in lightmapsData) { var texture = data.lightmapColor; path = AssetDatabase.GetAssetPath(texture); lightmapsInfo.Add(path, texture); } int[] numbers = { 1, 2, 3, 4, 5 }; foreach (int num in numbers){ Debug.Log(num); // Output: 1, 2, 3, 4, 5 } LightmapsBuffer Cache # In Unity\u0026rsquo;s lighting baking process, we need to cache, synthesize and save different types of lighting textures.\nLightmapsBuffer: Role and Implementation of Cache # Cache (Buffer) # Can be understood as a \u0026ldquo;storage box\u0026rdquo; used to temporarily save different types of lighting textures, convenient for subsequent processing and transfer.\nEnum # Used to define a set of constants, making code clearer and easier to understand.\npublic enum BufferType { MainLight, // Main light lighting: BufferA SkyLight, // Sky light lighting: BufferB EmissionGI, // Emission GI: BufferC Lightmap // Synthesized Lightmap } // Create cache for different types of lighting respectively private Texture2D[] _bufferA; // Main light private Texture2D[] _bufferB; // Sky light private Texture2D[] _bufferC; // Emission private Texture2D[] _lightmap; // Final synthesis Clear Cache (ClearBuffer \u0026amp; Clear) # switch-case: Execute different clearing logic according to different types (BufferType). void(type parameter name): If it\u0026rsquo;s a single data, parameter name is the temporary name of this data in the function If it\u0026rsquo;s a collection, parameter name is the temporary name of this collection in the function If it\u0026rsquo;s an enum, parameter name is the temporary name of elements in the enum. Write Cache (WriteBuffer) # First judge the type, if it\u0026rsquo;s the final Lightmap then return directly. Clear the corresponding type cache. Create new cache and copy textures from info. if (type == BufferType.Lightmap) return; // Clear cache ClearBuffer(type); // Create cache and copy textures from info var lightmapsCount = info.lightmapsCount; var buffer = new Texture2D[lightmapsCount]; for (var i = 0; i \u0026lt; lightmapsCount; i++) { var lightmap = info.lightmapsInfo.Values.ElementAt(i); buffer[i] = new Texture2D(lightmap.width, lightmap.height, lightmap.format, false); Graphics.CopyTexture(lightmap, 0, 0, buffer[i], 0, 0); } info.lightmapsInfo.Values.ElementAt(i): Take out the i-th texture from the dictionary in order.\nTexture2D(texture width, height, format (default RGB24), whether to generate mipmap)\nGraphics.CopyTexture: Unity API, used for efficient texture data copying.\nSynthesize Lightmap (CreateLightmaps) # Read pixels from main light, sky light, emission three types of cache in sequence, synthesize to a new Lightmap. for (var x = 0; x \u0026lt; width; x++) { for (var y = 0; y \u0026lt; height; y++) { var colA = _bufferA[i].GetPixel(x, y); var colB = _bufferB[i].GetPixel(x, y); var colC = _bufferC[i].GetPixel(x, y); var newCol = new Color(colA.r, colB.g, colC.b, 1.0f); lightmap.SetPixel(x, y, newCol.linear); } } Each pixel point takes corresponding colors from three cache textures, synthesizes to the final Lightmap. Note: Cache saves Texture2D objects, need to read and write pixel by pixel. Save Lightmap (OverrideLightmaps) # Encode the synthesized Lightmap to EXR format (supports HDR) and write to disk. Refresh Unity resource database to make new files recognized. for (var i = 0; i \u0026lt; lightmapsCount; i++) { var bytes = _lightmap[i].EncodeToEXR(Texture2D.EXRFlags.CompressZIP); File.WriteAllBytes(lightmapsInfo.Keys.ElementAt(i), bytes); AssetDatabase.Refresh(); } EncodeToEXR: Unity API, encodes texture to EXR format, EXR is Unity lighting system\u0026rsquo;s dedicated format Texture2D.EXRFlags.CompressZip compress to zip File.WriteAllBytes: .NET API, writes byte array to file. lightmapsInfo.Keys.ElementAt(i) get the i-th key from dictionary lightmapsInfo, i.e., path AssetDatabase.Refresh(): Refresh resource database (only available in editor mode). Baking Mode (BakeMode) and Scene Settings (ArrangeBakeScene) # BakeMode: Use enum to define different baking modes (all, main light, sky light, emission). ArrangeBakeScene: Set scene parameters according to different modes, such as ambient light type and intensity. RenderSettings.ambientMode = AmbientMode.Skybox; RenderSettings.ambientIntensity = 1.0f; ambientMode determines ambient light type (skybox, gradient, single color). ambientIntensity controls ambient light intensity (effective after baking). Corresponds to lightingSetting → lighting → Scene → Environment Lighting → Source \u0026amp; Intensity Multiplier\nSet main light as static:\nvar staticFlags = StaticEditorFlags.ContributeGI | StaticEditorFlags.ReflectionProbeStatic; GameObjectUtility.SetStaticEditorFlags(mainlight.gameObject, staticFlags); Execute Baking (Bake) and Reflection Probe Baking (BakeReflectProbe) # Bake directly calls Unity API for lighting baking.\npublic void Bake(BakeMode mode) { Lightmapping.Clear(); Lightmapping.Bake(); } BakeReflectProbe traverses all reflection probes in the scene, bakes and saves them one by one.\nprivate void BakeReflectProbe() { var allProbe = FindObjectsOfType\u0026lt;ReflectionProbe\u0026gt;(); foreach (var probe in allProbe) { var path = AssetDatabase.GetAssetPath(probe.texture); Lightmapping.BakeReflectionProbe(probe, path); } AssetDatabase.Refresh(); } FindObjectsOfType\u0026lt;\u0026gt;(): Find all reflection probes in the scene. AssetDatabase.GetAssetPath() get these reflection probes\u0026rsquo; textures and paths Lightmapping.BakeReflectionProbe(): Unity API, bake reflection probe. If you don\u0026rsquo;t customize storage path, reflection probe baked Cubemap defaults to LightingData\nLightingControllerGUI: Custom Baking Panel # After understanding the baking process and methods, we also need a custom UI panel to make baking operations and parameter adjustments more intuitive. Below introduces the implementation ideas and key code of LightingControllerGUI.\nBasic Structure of Editor Class # UnityEditor.Editor: Custom editor class must inherit from Editor and implement OnInspectorGUI() method. OnInspectorGUI(): Used to draw the content of Inspector panel. public class LightingControllerGUI : Editor { public override void OnInspectorGUI() { var controller = target as LightingController; if (controller == null) return; DrawFunctionButtons(controller); DrawGlobalProperties(controller); } } override when methods in class are set as virtual or abstract, can only use override to reference and override execution content target as LightingController: Get the object bound to current Inspector, must be MonoBehaviour derived class. DrawFunctionButtons: Draw baking-related operation buttons. DrawGlobalProperties: Draw global parameter adjustment controls. Drawing Function Buttons # Use GUILayout.Button(\u0026quot;button name\u0026quot;) to create buttons, execute corresponding methods when clicked. EditorGUILayout.BeginHorizontal() and EndHorizontal() are used to make multiple buttons display in the same row. if (GUILayout.Button(\u0026#34;Forbidden Art·Multiple Baking\u0026#34;)) controller.MultiBake(); EditorGUILayout.BeginHorizontal(); // Can add more buttons here EditorGUILayout.EndHorizontal(); Global Parameter Adjustment and Monitoring # Use EditorGUILayout.BeginFoldoutHeaderGroup to create collapsible parameter groups. Use EditorGUILayout.Slider to create draggable sliders for easy parameter adjustment. Use EditorGUI.BeginChangeCheck() and EndChangeCheck() to monitor parameter changes, automatically update when changed. EditorGUI.BeginChangeCheck(); _groupAToggle = EditorGUILayout.BeginFoldoutHeaderGroup(_groupAToggle, \u0026#34;Material Properties\u0026#34;); if (_groupAToggle) //Equivalent to if (groupAToggle == true) { controller.metalDarken = EditorGUILayout.Slider( \u0026#34;Metal Darkening\u0026#34;, controller.metalDarken, 0.0f, 5.0f); } EditorGUILayout.EndFoldoutHeaderGroup(); if (EditorGUI.EndChangeCheck()) { controller.UpdateGlobalProperties(); EditorUtility.SetDirty(controller); } _groupAToggle: Used to control the expand/collapse of foldout group, defaults to false when not assigned. EditorUtility.SetDirty(controller): Mark object as \u0026ldquo;modified\u0026rdquo;, ensure parameter changes can be recorded by Unity. EmissionShaderGUI: Custom Emission Parameter Panel # Through custom ShaderGUI, we can make the material sphere\u0026rsquo;s Inspector panel appear \u0026ldquo;Enable Emission GI\u0026rdquo; option, convenient for controlling whether emission participates in global illumination (GI).\nBasic Structure # Inherit ShaderGUI # When customizing material Inspector panel, need to inherit ShaderGUI class.\npublic class EmissionShaderGUI : ShaderGUI { // Specific implementation see below } After inheritance, Unity will automatically get the Shader and Material information using this script.\nGet Current Material # Get the current material instance being edited through materialEditor.target as Material.\nvar material = materialEditor.target as Material; Use Default Inspector Interface # If you don\u0026rsquo;t need to customize complex UI, you can directly call the base class\u0026rsquo;s OnGUI, this will display the default property panel.\nbase.OnGUI(materialEditor, properties); Add Emission GI Switch # Use EditorGUILayout.Toggle to create a checkbox, control whether emission participates in global illumination.\nToggle has 4 parameters\nlabel text content value condition style button style option button size var ifEmissionGIOn = EditorGUILayout.Toggle( \u0026#34;Enable Emission GI\u0026#34;, material.globalIlluminationFlags == MaterialGlobalIlluminationFlags.AnyEmissive); material.globalIlluminationFlags = ifEmissionGIOn ? MaterialGlobalIlluminationFlags.AnyEmissive : MaterialGlobalIlluminationFlags.EmissiveIsBlack; EditorGUILayout.Toggle(\u0026quot;interface text\u0026quot;, condition): Display checkbox state according to condition. Ternary operator condition ? resultA : resultB, equivalent to if-else statement: if (ifEmissionGIOn) material.globalIlluminationFlags = MaterialGlobalIlluminationFlags.AnyEmissive; else material.globalIlluminationFlags = MaterialGlobalIlluminationFlags.EmissiveIsBlack; Shader Explanation and Case Analysis # In this case, Shader has several key differences compared to other cases:\nLightmap UV calculation method MetaPass implementation Branch declaration (shader_feature) Custom ShadowCaster Standard Lightmap UV Calculation # In Unity, Lightmap UV coordinates are usually calculated like this:\nfloat2 lmUV = v.uv1.xy * unity_LightmapST.xy + unity_LightmapST.zw; uv1 is the model\u0026rsquo;s second set of UV (specifically for lighting textures). unity_LightmapST is the scale/offset parameter automatically passed by Unity. MetaPass: Shading Channel Dedicated to Baking # MetaPass is Unity lighting baking system\u0026rsquo;s dedicated Shader channel. Only Shaders with MetaPass can participate in lighting baking.\nCase Code # //Building.Shader\u0026#39;s MetaPass Pass { Name \u0026#34;META\u0026#34; Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;Meta\u0026#34; } CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;UnityMetaPass.cginc\u0026#34; #pragma shader_feature __ _BAKE_MAINLIGHT _BAKE_SKYLIGHT _BAKE_EMISSIONGI struct VertexInput { float4 vertex : POSITION; float2 uv0 : TEXCOORD0; float2 uv1 : TEXCOORD1; float2 uv2 : TEXCOORD2; }; struct VertexOutput { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; }; VertexOutput vert (VertexInput v) { VertexOutput o = (VertexOutput)0; o.pos = UnityMetaVertexPosition(v.vertex, v.uv1, v.uv2, unity_LightmapST, unity_DynamicLightmapST); o.uv = v.uv0; return o; } float4 frag(VertexOutput i) : COLOR { UnityMetaInput metaIN; UNITY_INITIALIZE_OUTPUT(UnityMetaInput, metaIN); metaIN.Albedo = Luminance(tex2D(_MainTex, i.uv).rgb); metaIN.SpecularColor = 0.0f; metaIN.Emission = 0.0f; return UnityMetaFragment(metaIN); } ENDCG } Key Points Explanation # UnityStandardMeta.cginc PBR process META Pass dedicated UnityMetaPass.cginc belongs to part of UnityStandardMeta.cginc, since the case is not PBR process, for optimization consideration, only reference this cginc. Compared to UnityStandardMeta.cginc need to customize vertex shader part UnityMetaVertexPosition: Convert vertex and UV to baking space. UnityMetaInput: Contains array of baking required data like Albedo, SpecularColor, Emission. UNITY_INITIALIZE_OUTPUT initialize UnityMetaInput Luminance: Convert color to grayscale, ensure baking only considers brightness, not affected by specular and emission. Branch Control (shader_feature) # //LightingController.cs ... case BakeMode.Default: // Turn off main light mainlight.enabled = true; // Set environment RenderSettings.ambientMode = AmbientMode.Skybox; RenderSettings.ambientIntensity = 1.0f; // Set Shader global branches Shader.DisableKeyword(\u0026#34;_BAKE_MAINLIGHT\u0026#34;); Shader.DisableKeyword(\u0026#34;_BAKE_SKYLIGHT\u0026#34;); Shader.DisableKeyword(\u0026#34;_BAKE_EMISSIONGI\u0026#34;); break; ... #pragma shader_feature __ _BAKE_MAINLIGHT _BAKE_SKYLIGHT _BAKE_EMISSIONGI Need to set global branches in both script and Shader Through branch declaration, can make Shader output different content for different baking modes. Use #if defined(...) for branch judgment in code: #if defined (_BAKE_EMISSIONGI) metaIN.Emission = opacity; #elif defined (_BAKE_MAINLIGHT) || defined (_BAKE_SKYLIGHT) metaIN.Emission = 0.0f; #endif Skybox Special Note # Skybox\u0026rsquo;s Shader doesn\u0026rsquo;t need MetaPass, Unity internally handles skybox lighting baking automatically. ShadowCaster: Custom Shadow Pass # Since this case scene is non-enclosed box-shaped and normals face inward, Unity\u0026rsquo;s default ShadowCaster cannot project correctly. Need to customize ShadowCaster Pass.\nCase Code # Pass { Name \u0026#34;ShadowCaster\u0026#34; Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ShadowCaster\u0026#34; } ZWrite On ZTest LEqual Cull off CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile_shadowcaster #include \u0026#34;UnityCG.cginc\u0026#34; struct v2f { V2F_SHADOW_CASTER; }; v2f vert(appdata_base v) { v2f o; TRANSFER_SHADOW_CASTER_NORMALOFFSET(o) return o; } float4 frag(v2f i) : SV_Target { SHADOW_CASTER_FRAGMENT(i) } ENDCG } Key Points Explanation # appdata_base: Unity predefined vertex input structure, contains POSITION, NORMAL, TEXCOORD0. V2F_SHADOW_CASTER: Output structure, contains shadow projection required clip space position and auxiliary vectors, equivalent to: float4 pos : SV_POSITION; float3 vec : TEXCOORD0; TRANSFER_SHADOW_CASTER_NORMALOFFSET(o): Calculate shadow offset, ensure shadow projection is correct. TRANSFER_SHADOW_CASTER_NORMALOFFSET(o) float4 clipPos = UnityClipSpaceShadowCasterPos(v.vertex, v.normal); \\ o.pos = UnityApplyLinearShadowBias(clipPos); \\ o.vec = ComputeOffsetData(clipPos); SHADOW_CASTER_FRAGMENT(i) Unity built-in macro, calculate shadow casting in fragment shader Cull off: Turn off culling, ensure inner surfaces can also project. uv0: TEXCOORD0 in VertexInput will automatically be recognized as the model\u0026rsquo;s first set of UV, in VertexOutput, TEXCOORD0 is a general interpolation register, it\u0026rsquo;s a developer-defined data channel, need to manually calculate and assign from input data in vert\nThe translation is now complete, including all Chinese comments and technical terms translated to English while maintaining the technical accuracy and context of the original content. ","date":"31 May 2025","externalUrl":null,"permalink":"/notes/courses/zhuangdongcourse/lightingmap/","section":"Notes","summary":"","title":"Zhuangdong_Course_Note_LightingMap","type":"notes"},{"content":" 13 Four Modes Related to Transparent Effects # 1. Alpha Blend (AB) # Usage # Objects with complex outlines and no clear edges Semi-transparent objects General base for effects Advantages # Good performance on mobile, natural edge effect\nDisadvantages # Has sorting issues\nKey Code # Shader \u0026#34;Zhuangdong/AP1/L07/L07_AB\u0026#34; { Properties{ _MainTex (\u0026#34;RGB: Color A: Alpha\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Opacity (\u0026#34;Opacity\u0026#34;, range(0,1)) = 0.5 } SubShader{ Tags { \u0026#34;Queue\u0026#34; = \u0026#34;Transparent\u0026#34; // Render transparent objects last to prevent objects behind from disappearing \u0026#34;RenderType\u0026#34; = \u0026#34;Transparent\u0026#34; \u0026#34;ForceNoShadowCasting\u0026#34; = \u0026#34;True\u0026#34; // Disable shadow casting \u0026#34;IgnoreProject\u0026#34; = \u0026#34;True\u0026#34; // Does not affect projector } Pass { Name \u0026#34;FORWARD\u0026#34; Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardBase\u0026#34; } Blend One OneMinusSrcAlpha // Declare blend mode CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #pragma multi_compile_fwdbase_fullshadows #pragma target 3.0 uniform sampler2D _MainTex; uniform float4 _MainTex_ST; uniform half _Opacity; struct VertexInput { float4 vertex : POSITION; float2 uv0 : TEXCOORD0; }; struct VertexOutput { float4 pos : SV_POSITION; float2 uv0 : TEXCOORD0; }; VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; o.pos = UnityObjectToClipPos(v.vertex); o.uv0 = TRANSFORM_TEX(v.uv0, _MainTex); return o; } half4 frag(VertexOutput i) : COLOR{ // Standard writing half4 var_MainTex = tex2D(_MainTex, i.uv0); half3 finalRGB = var_MainTex.rgb; half opacity = var_MainTex.a * _Opacity; return half4(finalRGB * opacity, opacity); } ENDCG } } FallBack \u0026#34;Diffuse\u0026#34; } 2. Additive (AD) # Usage # Luminous objects, glow effects Brightening effects Disadvantages # Has sorting issues Multiple layers easily cause overdraw For glow effects, usually can be replaced by post-processing Key Code # // Use AB example as template ... SubShader{ ... pass{ ... Blend One One // Just change the BlendMode from the AB example ... } ... } 3. Alpha CutOut (AC) # Usage # Complex outlines, clear edges (e.g. leaves, hair, metal hollow-outs) Cartoon effects (with anti-aliasing) Advantages # No sorting issues\nDisadvantages # Edges are harsh, poor performance on mobile\nKey Code # // Use AB example as template ... SubShader{ Tags{ \u0026#34;RenderType\u0026#34; = \u0026#34;TransparentCutout\u0026#34; // Corresponds to AC \u0026#34;ForceNoShadowCasting\u0026#34; = \u0026#34;True\u0026#34; \u0026#34;IgnoreProject\u0026#34; = \u0026#34;True\u0026#34; } pass{ ... half4 frag(VertexOutput i) : COLOR{ half4 var_MainTex = tex2D(_MainTex, i.uv0); half opacity = var_MainTex.a; clip(opacity - _Cutoff); // Alpha cutout return half4(var_MainTex.rgb, 1.0); } } ... } 4. Custom Blending # Usage # Custom blend formulas for flexible special effects\nCombination # Src * SrcFactor op Dst * DstFactor\nElements # Src: Source, the current shader drawing result Dst: Destination, the background before the current shader draws SrcFactor: Source multiplier DstFactor: Destination multiplier op: Blend operator The multipliers determine how the elements participate in blending, and the operator determines the form of blending. For details, please refer to the Unity official documentation.\nKey Code # ... Pass { Name \u0026#34;FORWARD\u0026#34; Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardBase\u0026#34; } BlendOp [_BlendOp] Blend [_BlendSrc] [_BlendDst] // Add the formula after Tags ... 15 Common Issues # 1. Sorting Issue # The front and back relationship is unclear when rendering transparent objects\nSolutions # Detach/Attach (modify model vertex order with DDC software) ZWrite Off (disable depth writing) 2. Premultiplied Texture Issue # Some texture assets are premultiplied (BaseColor * Alpha), some are not, so the blend mode needs to be distinguished\nAB mode: For premultiplied, use One OneMinusSrcAlpha; for non-premultiplied, use SrcAlpha OneMinusSrcAlpha, or multiply in frag AD mode: Premultiplied can have no A channel, non-premultiplied needs to multiply in frag 3. Example: GhostFlow # // Use AB as template Shader \u0026#34;Zhuangdong/AP1/L08/L08_GhostFlow\u0026#34; { Properties{ _MainTex (\u0026#34;RGB: Color A: Alpha\u0026#34;, 2D) = \u0026#34;gray\u0026#34; {} _Opacity (\u0026#34;Opacity\u0026#34;, range(0,1)) = 0.5 _WarpTex (\u0026#34;Distortion Map\u0026#34;, 2D) = \u0026#34;gray\u0026#34; {} _WarpInt (\u0026#34;Distortion Intensity\u0026#34;, range(0, 1)) = 0.5 _NoiseInt(\u0026#34;Noise Intensity\u0026#34;, range(0, 5)) = 0.5 _FlowSpeed (\u0026#34;Flow Speed\u0026#34;, range(0, 10)) = 5 } SubShader{ ... Pass { ... Blend One OneMinusSrcAlpha ... struct VertexInput { float4 vertex : POSITION; float2 uv : TEXCOORD0; }; struct VertexOutput { float4 pos : SV_POSITION; float2 uv0 : TEXCOORD0; float2 uv1 : TEXCOORD1; }; VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; o.pos = UnityObjectToClipPos(v.vertex); o.uv0 = v.uv; o.uv1 = TRANSFORM_TEX(v.uv, _WarpTex); // Enable Tiling o.uv1.y = o.uv1.y + frac(_Time.x * _FlowSpeed); // Start V axis flow return o; } half4 frag(VertexOutput i) : COLOR{ half3 var_WarpTex = tex2D(_WarpTex, i.uv1); half2 uvBias = (var_WarpTex.rg - 0.5) * _WarpInt; half2 uv0 = i.uv0 + uvBias; half4 var_MainTex = tex2D(_MainTex, uv0); half3 finalRGB = var_MainTex.rgb; half noise = lerp(1.0, var_WarpTex.b * 2.0, _NoiseInt); noise = max(0.0, noise); half opacity = var_MainTex.a * _Opacity * noise; return half4(finalRGB * opacity, opacity); } ENDCG } } FallBack \u0026#34;Diffuse\u0026#34; } Core Idea # By using a distortion map and flow speed parameter, achieve dynamic changes in transparent effects\no.uv1.y = o.uv1.y + frac(_Time.x * _FlowSpeed); // V axis flow half opacity = var_MainTex.a * _Opacity * noise; _Time: A parameter that increases with time, with x, y, z, w components representing different speed levels _FlowSpeed controls the flow speed frac takes the fractional part, used here to prevent _Time from growing indefinitely and causing moiré patterns noise = lerp(1.0, var_WarpTex.b * 2.0, _NoiseInt) ensures brightness does not darken _NoiseInt The higher this value, the clearer the noise; when above 1, noise contrast increases max(0.0, noise) prevents negative values that cause color anomalies 4. Example: GhostWarp # // Use GhostFlow as template ... half3 var_WarpTex = tex2D(_WarpTex, i.uv1); half2 uvBias = (var_WarpTex.rg - 0.5) * _WarpInt; half2 uv0 = i.uv0 + uvBias; half4 var_MainTex = tex2D(_MainTex, uv0); ... Principle # The rg channels of WarpTex control the direction of UV distortion, and -0.5 ensures even distribution of distortion\nNote # When WarpInt is too large, the distortion will be abrupt. This is caused by large grayscale differences between the bright and dark sides\n14, 16 Flame and Water Ripple Effect Cases # Flame Effect # Key Code # // Use GhostFlow as template Shader \u0026#34;Zhuangdong/AP1/L09/L09_Fire\u0026#34; { Properties{ _Mask (\u0026#34;R:Outer Flame G:Inner Flame B:Alpha\u0026#34;, 2D) = \u0026#34;blue\u0026#34; {} _Noise (\u0026#34;R:Noise1 G:Noise2\u0026#34;, 2D) = \u0026#34;gray\u0026#34; {} _Noise1Params (\u0026#34;Noise1 x:Scale y:Speed z:Intensity\u0026#34;, vector) = (1.0, 0.2, 0.2, 1.0) _Noise2Params (\u0026#34;Noise2 x:Scale y:Speed z:Intensity\u0026#34;, vector) = (1.0, 0.2, 0.2, 1.0) _color1 (\u0026#34;Outer Flame Color\u0026#34;, Color) = (1,1,1,1) _color2 (\u0026#34;Inner Flame Color\u0026#34;, Color) = (1,1,1,1) } SubShader{ ... Pass { ... Blend One OneMinusSrcAlpha ... struct VertexInput { float4 vertex : POSITION; float2 uv : TEXCOORD0; }; struct VertexOutput { float4 pos : SV_POSITION; float2 uv0 : TEXCOORD0; float2 uv1 : TEXCOORD1; float2 uv2 : TEXCOORD2; }; VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; o.pos = UnityObjectToClipPos(v.vertex); o.uv0 = v.uv; o.uv1 = v.uv * _Noise1Params.x - float2(0.0, frac(_Time.x * _Noise1Params.y)); o.uv2 = v.uv * _Noise2Params.x - float2(0.0, frac(_Time.x * _Noise2Params.y)); return o; } half4 frag(VertexOutput i) : COLOR{ half warpMask = tex2D(_Mask, i.uv0).b; half var_Noise1 = tex2D(_Noise, i.uv1).r; half var_Noise2 = tex2D(_Noise, i.uv2).g; half noise = var_Noise1 * _Noise1Params.z + var_Noise2 * _Noise2Params.z; float2 warpUV = i.uv0 - float2(0.0, noise) * warpMask; half3 var_Mask = tex2D(_Mask, warpUV); half3 finalRGB = _color1 * var_Mask.r + _color2 * var_Mask.g; half opacity = var_Mask.r + var_Mask.g; return half4(finalRGB, opacity); } ENDCG } } FallBack \u0026#34;Diffuse\u0026#34; } Principle # Two layers of noise overlay to control flame dynamics, and use mask to customize the color of inner and outer flames\nKey Points # _NoiseParams.x controls tiling _NoiseParams.y controls speed _NoiseParams.z controls distortion intensity float2(0,0, frac()) If v.uv - frac(), the texture will flow diagonally, because frac is automatically recognized as float2(frac(), frac()) Water Ripple Effect # Key Code # // Use Fire as template Shader \u0026#34;Zhuangdong/AP1/L09/L09_Water\u0026#34; { Properties{ _MainTex (\u0026#34;Color Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _WarpTex (\u0026#34;Distortion Map\u0026#34;, 2D) = \u0026#34;gray\u0026#34; {} _Speed (\u0026#34;x: Speed x y: Speed y\u0026#34;, vector) = (1.0, 1.0, 0.5, 1.0) _Warp1Params (\u0026#34;Noise1 x: Scale y: Speed x z: Speed y w: Intensity\u0026#34;, vector) = (1.0, 0.2, 0.2, 1.0) _Warp2Params (\u0026#34;Noise2 x: Scale y: Speed x z: Speed y w: Intensity\u0026#34;, vector) = (1.0, 0.2, 0.2, 1.0) } SubShader{ Tags { \u0026#34;RenderType\u0026#34; = \u0026#34;Opaque\u0026#34; } Pass { Name \u0026#34;FORWARD\u0026#34; Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardBase\u0026#34; } ... VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; o.pos = UnityObjectToClipPos(v.vertex); o.uv0 = v.uv - frac(_Time.x * _Speed); // Main texture flow o.uv1 = v.uv * _Warp1Params.x - frac(_Time.x * _Warp1Params.yz); // Distortion map 1 flow o.uv2 = v.uv * _Warp2Params.x - frac(_Time.x * _Warp2Params.yz); // Distortion map 2 flow return o; } float4 frag(VertexOutput i) : COLOR{ half3 var_Warp1 = tex2D(_WarpTex, i.uv1).rgb; // Distortion value 1 half3 var_Warp2 = tex2D(_WarpTex, i.uv2).rgb; // Distortion value 2 half2 warp = (var_Warp1.xy - 0.5) * _Warp1Params.w + (var_Warp2.xy - 0.5) * _Warp2Params.w; float2 warpUV = i.uv0 + warp; // Add distortion value half4 var_MainTex = tex2D(_MainTex, warpUV); return float4(var_MainTex.xyz, 1.0); } ENDCG } } FallBack \u0026#34;Diffuse\u0026#34; } Principle # Main texture UV plus two layers of noise to achieve water surface ripple effect\n17 ScreenUV \u0026amp; ScreenWarp # ScreenUV # By sampling in screen space UV, achieve effects where the texture changes and flows with camera distance.\nThe key is to use view space coordinates to correct distortion and overlay flow effects.\nKey Code # // Use AB as template ... VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; o.pos = UnityObjectToClipPos(v.vertex); o.uv = v.uv; float3 posVS = UnityObjectToViewPos(v.vertex).xyz; // Vertex position OS to VS o.screenUV = posVS.xy / posVS.z; // VS space distortion correction float originDist = UnityObjectToViewPos(float3(0.0, 0.0, 0.0)).z; // Origin position o.screenUV *= originDist; // o.screenUV * originDist o.screenUV = o.screenUV * _ScreenTex_ST.xy - frac(_Time.x * _ScreenTex_ST.zw); // Add tiling, offset, and flow return o; } half4 frag(VertexOutput i) : COLOR{ half4 var_MainTex = tex2D(_MainTex, i.uv); half var_ScreenTex = tex2D(_ScreenTex, i.screenUV).r; half3 finalRGB = var_MainTex.rgb; half opacity = var_MainTex.a * _Opacity * var_ScreenTex; return half4(finalRGB * opacity, opacity); } ... Key Points # o.screenUV view space corrected UV, prevents depth distortion originDist controls texture scaling by origin distance time overlays flow effect ScreenWarp # Use GrabPass to get the background, and use a channel of the main texture to distort the screen UV, achieving background distortion in semi-transparent areas (similar to Photoshop overlay).\nKey Code # Shader \u0026#34;Zhuangdong/AP1/L10/L10_ScreenWarp\u0026#34; { Properties{ _MainTex (\u0026#34;RGB: Color A: Alpha\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Opacity (\u0026#34;Opacity\u0026#34;, range(0,1)) = 0.5 _WarpMidVal (\u0026#34;Distortion Mid Value\u0026#34;, range(0, 1)) = 0.5 _WarpInt (\u0026#34;Distortion Intensity\u0026#34;, range(0, 3)) = 0.2 } SubShader{ ... GrabPass{ \u0026#34;_BGTex\u0026#34; } ... uniform sampler2D _BGTex; ... struct VertexOutput { float4 pos : SV_POSITION; float2 uv0 : TEXCOORD0; float4 grabPos : TEXCOORD1; // Background sampling coordinates }; VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; o.pos = UnityObjectToClipPos(v.vertex); o.uv0 = TRANSFORM_TEX(v.uv0, _MainTex); o.grabPos = ComputeGrabScreenPos(o.pos); // Background texture sampling coordinates for grabpass return o; } half4 frag(VertexOutput i) : COLOR{ half4 var_MainTex = tex2D(_MainTex, i.uv0); i.grabPos.xy += (var_MainTex.b - _WarpMidVal) * _WarpInt * _Opacity; half3 var_BGTex = tex2Dproj(_BGTex, i.grabPos); // tex2Dproj is a command specifically for sampling this type of texture half3 finalRGB = var_MainTex.rgb * var_BGTex; half opacity = var_MainTex.a * _Opacity; return half4(finalRGB * opacity, opacity); ... Key Points # GrabPass gets the background and saves it in BGTex WarpMidVal adjusts the UV sampling position _Opacity is bound to i.grabPos, the lower the opacity, the more obvious the distortion Similar Methods # GrabPass: Suitable for high quality but high performance cost CommandBuffer: Custom pipeline method before SRP era Lwrp/Urp: Custom pipeline method after SRP era (recommended) 18. Sequence Animation (Sequence) # Principle of Sequence Animation # In effect production, a sequence texture containing multiple frames is often used, with each frame representing a state of the animation. By switching the UV sampling area, the animation playback effect is achieved.\nThe sample texture is 3 rows and 4 columns, with each frame arranged in order. The effect layer floats above the object surface (extruded along the normal direction by the vertex). The UV origin is at the top left, and the initial sampling area needs to be adjusted. Key Code # // Add a pass based on the AB example ... pass{ Name \u0026#34;FORWARD\u0026#34; Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardBase\u0026#34; } Blend One One ... VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; // Extrude vertex along normal direction v.vertex.xyz += v.normal * 0.01; o.pos = UnityObjectToClipPos(v.vertex); o.uv = TRANSFORM_TEX(v.uv, _Sequence); // Calculate current frame index float id = floor(_Time.z * _Speed); float idV = floor(id / _ColCount); float idU = id - idV * _ColCount; float stepU = 1.0 / _ColCount; float stepV = 1.0 / _RowCount; // UV scaling and adjust origin to top left float2 initUV = o.uv * float2(stepU, stepV) + float2(0.0, stepV * (_RowCount -1)); o.uv = initUV + float2(idU * stepU, -idV * stepV); return o; } half4 frag(VertexOutput i) : COLOR{ half4 var_Sequence = tex2D(_Sequence, i.uv); half3 finalRGB = var_Sequence.rgb; half opacity = var_Sequence.a; return half4(finalRGB * opacity, opacity); } ENDCG ... Key Points Summary # floor rounds down, e.g. 1.9 outputs 1 id moves horizontally by 1/_ColCount each time idV = floor(id / _ColCount) when id is an integer multiple of the column count, idV increases by 1 idU = id - idV * _ColCount moves vertically by 1/_RowCount for each full row, and idU resets to zero float2(idU * stepU, -idV * stepV) moves by 1/_ColCount each time, when the number of moves equals _ColCount, idU resets to the leftmost, and moves vertically by 1/_RowCount Note: Effect algorithms should be as simple as possible to avoid overdraw.\nPolar Coordinate Animation (PolarCoord) # Polar coordinate transformation can achieve radial flow, scanning and other effects. By converting UV from Cartesian coordinates to polar coordinates and overlaying time flow, special animation effects are achieved.\nPolar Coordinates # Assume an x-axis, connect the origin to point M, the angle between OM and the x-axis is θ, OM length is P, polar coordinates are (θ, P)\nKey Code # half4 frag(VertexOutput i) : COLOR { i.uv = i.uv - float2(0.5, 0.5); float theta = atan2(i.uv.y, i.uv.x); theta = theta / 3.1415926 * 0.5 + 0.5; float p = length(i.uv) + frac(_Time.x * 3); i.uv = float2(theta, p); half4 var_MainTex = tex2D(_MainTex, i.uv); half3 finalRGB = (1 - var_MainTex.rgb); half opacity = (1 - var_MainTex.r) * _Opacity * i.color.r; return half4(finalRGB * opacity, opacity); } Key Points Summary # atan2 calculates angle θ, result is (-π, π), normalized to [0,1]. length calculates the distance from point (x, y) to the origin. Since the origin is shifted from (0,0) to (0.5, 0.5), the output will look like this float2(theta, p) maps UV to polar coordinates to achieve radial animation. i.color vertex color is used to soften the edge. 19. Vertex Animation # 19.1 Translation # Use sine function to make the vertex move periodically in the Y axis direction, achieving an overall up and down floating effect.\nKey Code # // Use AB as template ... #define TWO_PI 6.283185 void Translation (inout float3 vertex) { vertex.y += _MoveRange * sin(frac(_Time.z * _MoveSpeed) * TWO_PI); } VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; Translation(v.vertex.xyz); o.pos = UnityObjectToClipPos(v.vertex); o.uv0 = v.uv0; return o; } Key Points # frac Fractional part, takes the decimal part, ensures the time parameter cycles in [0,1], prevents overflow. sin sin(0, 2π) is a periodic motion from 0 to 1 and back to 0. 19.2 Scale # All vertices are scaled by the same ratio.\nKey Code # // Use AB as template void Scaling (inout float3 vertex) { vertex.xyz *= 1.0 + _ScaleRange * sin(frac(_Time.z * _ScaleSpeed) * TWO_PI); } VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; Scaling(v.vertex.xyz); o.pos = UnityObjectToClipPos(v.vertex); o.uv0 = TRANSFORM_TEX(v.uv0, _MainTex); return o; } Key Points # Ensure the scale value is greater than 0 to avoid reverse scaling.\n19.3 Rotation # Vertices rotate periodically around the Y axis, achieving effects like head shaking and swinging.\nKey Code # // Use AB as template void Rotation (inout float3 vertex) { float angleY = _RotateRange * sin(frac(_Time.z * _Rotatepeed) * TWO_PI); float radY = radians(angleY); float sinY, cosY = 0; sincos(radY, sinY, cosY); vertex.xz = float2( vertex.x * cosY - vertex.z * sinY, vertex.x * sinY + vertex.z * cosY ); } Key Points # radians converts degrees to radians, which improves performance sincos(raY, sinY, cosY) is equivalent to sinY = sin(radY), cosY = cos(radY), this writing improves performance. vertex.xz rotation matrix M (x * cosθ - y * sinθ, x * sinθ + y * cosθ) 19.4 Composite Animation (AnimationGhost) # Combine scaling, translation, rotation and other animations, and use vertex color to achieve complex movements (such as ghosts, angel rings, etc).\nKey Code # ... // Use AB as template void AnimGhost (inout float3 vertex, inout float3 color){ // 天使圈缩放 float scale = _ScaleParams.x * color.g * sin(frac(_Time.z * _ScaleParams.y) * TWO_PI); vertex.xyz *= 1.0 + scale; vertex.y -= _ScaleParams.z * -scale; // 幽灵摆动 float swingX = _SwingXparams.x * sin(frac(_Time.z * _SwingXparams.y + vertex.y * _SwingXparams.z) * TWO_PI); float swingZ = _SwingZparams.x * sin(frac(_Time.z * _SwingZparams.y + vertex.y * _SwingZparams.z) * TWO_PI); vertex.xz += float2(swingX, swingZ) * color.r; // 幽灵摇头 float radY = radians(_ShakeYparams.x) * (1.0 - color.r) * sin(frac(_Time.z * _ShakeYparams.y - color.g * _ShakeYparams.z) * TWO_PI); float sinY, cosY = 0; sincos(radY, sinY, cosY); vertex.xz = float2( vertex.x * cosY - vertex.z * sinY, vertex.x * sinY + vertex.z * cosY ); // 幽灵起伏 float swingY = _SwingYparams.x * sin(frac(_Time.z * _SwingYparams.y - color.g * _SwingYparams.z) * TWO_PI); vertex.y += swingY; // 处理顶点色 float lightness = 1.0 + color.g * 1.0 + scale * 2.0; color = float3(lightness, lightness, lightness); } ... Key Points # Use vertex color R, G channels to control different animation areas. vertex.y -= _ScaleParams.z * -scale is to scale around the model origin, control y axis scale amplitude, avoid angel ring leaving origin vertex.y * _SwingXparams.z sin result is affected by vertex.y, achieve S-shaped swing _ShakeYoarams.z _SwingYparams.z make angel ring and other parts of animation, produce time lag lightness brightness changes with time 20. Clock Animation (ClockAnim) # 20.1 Clock Pointer Animation # By C# script obtaining system time, driving Clock pointer rotation in Shader, achieving real clock animation.\nKey Code # ... void RotateZwithOffset(float angle, float offset, float mask, inout float3 vertex){ vertex.y -= offset * mask; float radZ = radians(angle * mask); float sinZ, cosZ = 0; sincos(radZ, sinZ, cosZ); vertex.xy = float2( vertex.x * cosZ - vertex.y * sinZ, vertex.x * sinZ + vertex.y * cosZ ); vertex.y += offset * mask; } void ClockAnim(float3 color, inout float3 vertex) { RotateZwithOffset(_HourHandAngle, _RotateOffset, color.r, vertex); RotateZwithOffset(_MinuteHandAngle, _RotateOffset, color.g, vertex); RotateZwithOffset(_SecondHandAngle, _RotateOffset, color.b, vertex); } VertexOutput vert(VertexInput v) { VertexOutput o = (VertexOutput)0; ClockAnim(v.color.rgb, v.vertex.xyz); ... Key Points # angle represents the rotation angle of each time unit offset adjusts the rotation center. Initial center is at model origin, needs adjustment C# script binding system time # using System; using UnityEngine; public class HelloWorld : MonoBehaviour { // --------- Public ------- public Material clockMat; // --------- private -------- private bool valid; private int hourAnglePropID; private int minuteAnglePropID; private int secondAnglePropID; // Start is called before the first frame update void Start() { if(clockMat == null) return; hourAnglePropID = Shader.PropertyToID(\u0026#34;_HourHandAngle\u0026#34;); minuteAnglePropID = Shader.PropertyToID(\u0026#34;_MinuteHandAngle\u0026#34;); secondAnglePropID = Shader.PropertyToID(\u0026#34;_SecondHandAngle\u0026#34;); if(clockMat.HasProperty(hourAnglePropID) \u0026amp;\u0026amp; clockMat.HasProperty(minuteAnglePropID) \u0026amp;\u0026amp; clockMat.HasProperty(secondAnglePropID)) valid = true; Debug.Log(\u0026#34;hourAnglePropID\u0026#34; + hourAnglePropID); Debug.Log(\u0026#34;minuteAnglePropID\u0026#34; + minuteAnglePropID); Debug.Log(\u0026#34;secondAnglePropID\u0026#34; + secondAnglePropID); Debug.Log(valid); } // Update is called once per frame void Update() { if(!valid) return; int second = DateTime.Now.Second; float secondAngle = second /60.0f * 360.0f; clockMat.SetFloat(secondAnglePropID, secondAngle); int minute = DateTime.Now.Minute; float minuteAngle = minute /60.0f * 360.0f; clockMat.SetFloat(minuteAnglePropID, minuteAngle); int hour = DateTime.Now.Hour; float hourAngle = (hour % 12) / 12.0f * 360.0f + minuteAngle / 360.0f * 30.0f; clockMat.SetFloat(hourAnglePropID, hourAngle); } } Key Points # % remainder, ensure hour over 12 to 0 20.2 Shadow Projection Pass # Key Code # Pass { Name \u0026#34;ShadowCaster\u0026#34; Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ShadowCaster\u0026#34; } CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile_shadowcaster #include \u0026#34;UnityCG.cginc\u0026#34; uniform float _HourHandAngle, _MinuteHandAngle, _SecondHandAngle, _RotateOffset; struct appdata { float4 vertex : POSITION; float3 normal : NORMAL; float3 color : COLOR; }; struct v2f { V2F_SHADOW_CASTER; }; void RotateZwithOffset(float angle, float offset, float mask, inout float3 vertex){ vertex.y -= offset * mask; float radZ = radians(angle * mask); float sinZ, cosZ = 0; sincos(radZ, sinZ, cosZ); vertex.xy = float2( vertex.x * cosZ - vertex.y * sinZ, vertex.x * sinZ + vertex.y * cosZ ); vertex.y += offset * mask; } void ClockAnim(float3 color, inout float3 vertex) { RotateZwithOffset(_HourHandAngle, _RotateOffset, color.r, vertex); RotateZwithOffset(_MinuteHandAngle, _RotateOffset, color.g, vertex); RotateZwithOffset(_SecondHandAngle, _RotateOffset, color.b, vertex); } v2f vert(appdata v) { v2f o; ClockAnim(v.color.rgb, v.vertex.xyz); // 关键修复：计算带法线偏移的阴影坐标 float3 posWS = mul(unity_ObjectToWorld, v.vertex).xyz;//动画后的顶点转世界空间 float3 normalWS = UnityObjectToWorldNormal(v.normal);//法线转世界空间 o.pos = UnityClipSpaceShadowCasterPos(posWS, normalWS); TRANSFER_SHADOW_CASTER_NORMALOFFSET(o) return o; } float4 frag(v2f i) : SV_Target { SHADOW_CASTER_FRAGMENT(i) } ENDCG } Key Points # ShadowCaster is fixed for projection Pass naming and LightMode\nUnityClipSpaceShadowCasterPos(posWS, normalWS) normal offset correction, get animation front normal and animation back vertex, calculate animation back normal direction, finally output animation back vertex position and corrected normal in light cone clipping space coordinates\nV2F_SHADOW_CASTER Unity built-in macro, equivalent to:\nfloat4 pos : SV_POSITION float3 vec : TEXCOORD0 (only used when there is point light) TRANSFER_SHADOW_CASTER_NORMALOFFSET(o) input posWS, normalWS, output V2F_SHADOW_CASTER\nSHADOW_CASTER_FRAGMENT(i) input pos, process projection according to light type\nNote: Only need ShadowCaster, V2F_SHADOW_CASTER, TRANSFER_SHADOW_CASTER_NORMALOFFSET(o), SHADOW_CASTER_FRAGMENT(i) 4 elements to get a complete shadow projection Pass\n21. Effect Dissolve Animation Case # 21.1 Gray Scale and Noise Control Dissolve # By multi-channel gray scale map, noise map, and vertex color, achieve complex dissolve effects like mesh disappearance, randomness, and light emission.\nKey Structure and Function # struct VertexInput { float4 vertex : POSITION; float2 uv0 : TEXCOORD0; float2 uv1 : TEXCOORD1; float4 normal : NORMAL; float4 tangent : TANGENT; float4 color : COLOR; }; struct VertexOutput { float4 pos : SV_POSITION; float2 uv0 : TEXCOORD0; float2 uv1 : TEXCOORD1; float4 posWS : TEXCOORD2; float3 nDirWS : TEXCOORD3; float3 tDirWS : TEXCOORD4; float3 bDirWS : TEXCOORD5; float4 effectMask : TEXCOORD6; LIGHTING_COORDS(7,8) }; float4 CyberpunkAnim(float noise, float mask, float3 normal, inout float3 vertex){ float baseMask = abs(frac(vertex.y * _EffParams.x - _Time.x * _EffParams.y) -0.5) * 2.0; baseMask = min(1.0, baseMask * 2.0); baseMask += (noise - 0.5) * _EffParams.z; float4 effectMask = float4(0.0, 0.0, 0.0, 0.0); effectMask.x = smoothstep(0.0, 0.9, baseMask); effectMask.y = smoothstep(0.2, 0.7, baseMask); effectMask.z = smoothstep(0.4, 0.5, baseMask); effectMask.w = mask; vertex.xz += normal.xz * (1.0 - effectMask.y) * _EffParams.w * mask; return effectMask; } Vertex Shader # VertexOutput vert(VertexInput v) { float noise = tex2Dlod(_EffectMap02, float4(v.uv1, 0.0, 0.0)).r; VertexOutput o = (VertexOutput)0; o.effectMask = CyberpunkAnim(noise, v.color, v.normal.xyz, v.vertex.xyz); o.pos = UnityObjectToClipPos(v.vertex); o.uv0 = v.uv0; o.uv1 = v.uv1; o.posWS = mul(unity_ObjectToWorld, v.vertex); o.nDirWS = UnityObjectToWorldNormal(v.normal); o.tDirWS = normalize( mul( unity_ObjectToWorld, float4( v.tangent.xyz, 0.0 ) ).xyz ); o.bDirWS = normalize(cross(o.nDirWS, o.tDirWS) * v.tangent.w); TRANSFER_VERTEX_TO_FRAGMENT(o) return o; } Fragment Shader # float4 frag(VertexOutput i) : COLOR{ // ...(向量、光照、采样等略)... //光照模型 float3 baseCol = var_MainTex.rgb * _BaseColor; float Lambert = max(0.0, nDotl);//lambert float specCol = var_SpecTex.rgb; float specpow = lerp(1, _SpecularPow_Value, var_SpecTex.a); float Phong = pow(max(0.0, vDotr), _SpecularPow_Value); float shadow = LIGHT_ATTENUATION(i); float3 dirlighting = (baseCol * Lambert + specCol * Phong) * _LightColor0 * shadow; float3 EnvCol = TriColAmbient(nDirWS, _TopCol, _MidCol, _BotCol); float fresnel = pow(1.0 - ndotv, _FresnelPow); float occlusion = var_MainTex.a; float3 envLighting = (baseCol * EnvCol * _EnvDiffInt + var_Cubemap * fresnel* _EnvSpecint * var_SpecTex.a) * occlusion; //自发光 float3 emission = var_EmitTex * _EmitInt; float3 _EffectMap01_var = tex2D(_EffectMap01, i.uv1).xyz; float meshMask = _EffectMap01_var.x; float faceRandomMask = max(0.001, _EffectMap01_var.y); float faceSlopMask = max(0.001, _EffectMap01_var.z); float smallMask = i.effectMask.x; float midMask = i.effectMask.y; float bigMask = i.effectMask.z; float baseMask = i.effectMask.w; float midOpacity = saturate(floor(min(faceRandomMask, 0.999999) + midMask)); float bigOpacity = saturate(floor(min(faceSlopMask, 0.999999) + bigMask)); float opacity = lerp(1.0, min(bigOpacity, midOpacity), baseMask); float meshEmitInt = (bigMask - smallMask) * meshMask;//只在半透明的区域有发光效果 meshEmitInt = meshEmitInt * meshEmitInt * 2.0;//进行固定值的 power，让发光区域缩小 emission += _EffCol * meshEmitInt * baseMask; float3 FinalColor = dirlighting + envLighting + emission; return float4(FinalColor * opacity, opacity); } Key Points # EffMap01 R - WireframeMap records model wire frame G - RandomGrayScale records random gray scale based on surface B - DisappearanceGrayscale records uniform depth based on surface EffMap02 3DPerlinNoise DisappearanceGrayscale\nRandomGrayScale\n贴图制作与合并\nmask vertex color, animate only on character baseMask = abs(frac(vertex.y * _EffParams.x - _Time.x * _EffParams.y) -0.5) * 2.0; frac(vertex.y) only takes the decimal, visual gradient Linear 1 EffectParams.x controls gradient Linear 1 Tiling EffectParams.y controls animation speed and direction abs(frac() -0.5) * 2.0 abs takes absolute value, finally result gradient Linear 1 to gradient Linear 3 baseMask = min() increases value range to 1 baseMask += (noise - 0.5) * _EffParams.z; makes baseMask value non 1 area change, _EffParams.z controls change intensity smoothstep adjusts specified area waveform vertex.xz += animate transparent area, EffParams.w controls animation intensity floor less than 1 to 0, greater than 1 to 1 saturate limits value range to (0, 1) meshEmitInt * meshEmitInt equivalent to Power = value^2 faceRandomMask = max(0.001, _EffectMap01_var.y) prevent negative, negative causes display error ","date":"27 May 2025","externalUrl":null,"permalink":"/notes/courses/zhuangdongcourse/vfxshader/","section":"Notes","summary":"","title":"Zhuangdong_Course_Note_VFXShader","type":"notes"},{"content":" 07-Dot Product and Duality # Geometric Meaning of Dot Product # The dot product of two vectors $\\vec{V}$ and $\\vec{W}$ equals the projection length of $\\vec{W}$ in the direction of $\\vec{V}$ multiplied by the length of $\\vec{V}$.\nFormula # $$ (x_1, y_1) \\cdot (x_2, y_2) = x_1x_2 + y_1y_2 $$\nSign of dot product result: If the projection direction of $\\vec{W}$ is the same as $\\vec{V}$, the dot product is positive. If the projection direction of $\\vec{W}$ is opposite to $\\vec{V}$, the dot product is negative. If $\\vec{W}$ is perpendicular to $\\vec{V}$, the dot product is zero. Dot product satisfies commutativity: $\\vec{V} \\cdot \\vec{W} = \\vec{W} \\cdot \\vec{V}$. Assuming W and V are two vectors of the same length, they are symmetric on a line. When V becomes 2 times longer, W\u0026rsquo;s projection on V remains unchanged, while V\u0026rsquo;s projection on W becomes 2 times longer. At this point:\n$2(\\vec{V}) \\cdot \\vec{W} = 2(\\vec{V} \\cdot \\vec{W})$\nDuality # The result of a 2D vector transformed by a $1 \\times 2$ matrix is the same as the dot product result of two 2D vectors. The process of one vector projecting onto another can be viewed as a linear transformation.\nExample: $[1, -2] \\begin{bmatrix}4 \\ 3\\end{bmatrix} = \\begin{bmatrix}1 \\ -2\\end{bmatrix} \\cdot \\begin{bmatrix}4 \\ 3\\end{bmatrix}$\nAssuming there exists a unit vector $\\vec{U}$, and the projections of basis vectors $i, j$ onto this vector are $U_x, U_y$, then the projection matrix is $[U_x, U_y]$. Any vector transformed by $[U_x, U_y]$ to that line, or dot product with $\\vec{U}$, gives the same result. The dot product of $\\vec{U}$ with unit vector $i$ equals the projection length of $\\vec{U}$ in the direction of $i$. If $\\vec{U}$ is scaled to 3 times its original size, like $[3U_x, 3U_y]$, the result equals the projection value multiplied by 3. This phenomenon is called \u0026ldquo;duality\u0026rdquo;, which is the natural correspondence between matrix-vector multiplication and dot product.\n08-Cross Product # Part One: Standard Introduction to Cross Product # Given vectors $\\vec{V}$ and $\\vec{W}$, translate them to each other\u0026rsquo;s endpoints to form a parallelogram plane. The cross product result is a vector perpendicular to this plane, with length equal to the plane area. The direction of the new vector produced by the cross product is determined by the right-hand rule.\nSign of Cross Product: # If $\\vec{V}$ is to the right of $\\vec{W}$, the result is positive. If $\\vec{V}$ is to the left of $\\vec{W}$, the result is negative. Formula # Two-dimensional\n$$ \\det\\left(\\begin{bmatrix}X_1 \u0026amp; Y_1 \\ X_2 \u0026amp; Y_2\\end{bmatrix}\\right) = X_1Y_2 - X_2Y_1 $$\nThree-dimensional\n$$ (V_1, V_2, V_3) \\times (W_1, W_2, W_3) = (V_2W_3 - V_3W_2, V_3W_1 - V_1W_3, V_1W_2 - V_2W_1 $$\nCharacteristics # The more perpendicular the two vectors are, the larger the area. If one vector is scaled, the area is also scaled proportionally. If the result is negative, it indicates the coordinate system orientation is flipped. Part Two: Cross Product from the Perspective of Linear Transformation # Define a linear transformation from 3D space to 1D: There exist basis vectors $\\vec{V}\\vec{W}$, and any vector $\\vec{U}$\n$$ \\vec{U} = \\begin{bmatrix}x \\ y \\ z\\end{bmatrix}, f((x, y, z)) = \\det\\left(\\begin{bmatrix}x \u0026amp; y \u0026amp; z \\ V_1 \u0026amp; V_2 \u0026amp; V_3 \\ W_1 \u0026amp; W_2 \u0026amp; W_3\\end{bmatrix}\\right) $$\nThere exists a vector $\\vec{P}$ such that: $$\\begin{bmatrix}P_1 \\ P_2 \\ P_3\\end{bmatrix} \\cdot \\begin{bmatrix}x \\ y \\ z\\end{bmatrix} = \\det\\left(\\begin{bmatrix}x \u0026amp; y \u0026amp; z \\ V_1 \u0026amp; V_2 \u0026amp; V_3 \\ W_1 \u0026amp; W_2 \u0026amp; W_3\\end{bmatrix}\\right)$$\nThe axis perpendicular to basis vectors $\\vec{V}\\vec{W}$ is $K$\nIn 3D space, the volume of a parallelepiped is determined by the projection length of $\\vec{U}$ on $K$ and the area of the plane formed by $\\vec{V}\\vec{W}$.\n$\\vec{P} \\cdot \\vec{U} = |\\vec{P}| \\times \\vec{U}_{\\text{\\scriptsize projected onto P}}$\n$|\\vec{P}| \\times \\vec{U}{\\text{\\scriptsize projected onto P}}$ = Area of plane formed by $\\vec{V}\\vec{W}$ × $\\vec{U}{\\text{\\scriptsize projected onto K}}$.\nFrom this, we can conclude that $\\vec{P}$ is perpendicular to the plane formed by $\\vec{V}\\vec{W}$, with length equal to the area of that plane, therefore the equation holds.\n","date":"25 May 2025","externalUrl":null,"permalink":"/notes/foundation/linearalgebra/essenceoflinearalgebra_part3/","section":"Notes","summary":"","title":"EssenceOfLinearAlgebra - 03","type":"notes"},{"content":" 09-Change of Basis # All the cases we discussed earlier were conducted in the coordinate system defined by the standard basis vectors $\\hat{i} = \\begin{bmatrix} 1 \\ 0 \\end{bmatrix}$ and $\\hat{j} = \\begin{bmatrix} 0 \\ 1 \\end{bmatrix}$.\nIn different coordinate systems, the coordinates describing the same vector are different, because the basis vectors we choose have changed.\nExample # Suppose there is another set of basis vectors $\\hat{i}\u0026rsquo; = \\begin{bmatrix} 2 \\ 1 \\end{bmatrix}$ and $\\hat{j}\u0026rsquo; = \\begin{bmatrix} -1 \\ 1 \\end{bmatrix}$.\nThe vector $\\begin{bmatrix} 5/3 \\ 1/3 \\end{bmatrix}$ in this coordinate system is represented as $\\begin{bmatrix} 3 \\ 2 \\end{bmatrix}$ in our standard coordinate system.\nCoordinate System Transformation # We can form the new basis vectors into a matrix $A = \\begin{bmatrix} 2 \u0026amp; -1 \\ 1 \u0026amp; 1 \\end{bmatrix}$. Multiplying a vector in another coordinate system by this matrix gives its representation in the standard coordinate system. $$\\begin{bmatrix} 2 \u0026amp; -1 \\ 1 \u0026amp; 1 \\end{bmatrix}\\begin{bmatrix} 5/3 \\ 1/3 \\end{bmatrix} = \\begin{bmatrix} 3 \\ 2 \\end{bmatrix}$$\nInverse Coordinate System Transformation # Simply take the inverse matrix $A^{-1}$ of the above matrix $A$. Multiplying a vector in the standard coordinate system by $A^{-1}$ gives its representation in another coordinate system. Case Study # How to implement the same transformation in two different coordinate systems? (e.g., counterclockwise rotation by 90°)\nSuppose $M$ is the transformation matrix in the standard coordinate system (rotation by 90°: $M = \\begin{bmatrix} 0 \u0026amp; -1 \\ 1 \u0026amp; 0 \\end{bmatrix}$). Suppose $A$ is the transformation matrix from another coordinate system to the standard coordinate system. The matrix to implement the same transformation in another coordinate system is $A^{-1}MA$. The intuitive understanding of this operation is:\n$A$: Transform the vector from another coordinate system to the standard coordinate system. $M$: Perform rotation in the standard coordinate system. $A^{-1}$: Transform the result back to the original coordinate system. For a coordinate system with basis vectors $\\begin{bmatrix} 2 \\ 1 \\end{bmatrix}$ and $\\begin{bmatrix} -1 \\ 1 \\end{bmatrix}$, its transformation matrix for rotation by 90° is: $A^{-1}MA = \\begin{bmatrix} 2 \u0026amp; -1 \\ 1 \u0026amp; 1 \\end{bmatrix} \\begin{bmatrix} 0 \u0026amp; -1 \\ 1 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} 1/3 \u0026amp; 1/3 \\ -1/3 \u0026amp; 2/3 \\end{bmatrix} = \\begin{bmatrix} 1/3 \u0026amp; -5/3 \\ 2/3 \u0026amp; -1/3 \\end{bmatrix}$\n10-Eigenvectors and Eigenvalues # Eigenvector # A non-zero vector that maintains its direction (only scales within its spanned space) when undergoing the linear transformation described by a matrix.\nEigenvalue # The scaling factor by which the eigenvector is stretched or compressed during the transformation.\nGeometrically, if a 3D rotation can be viewed as occurring around an axis, then the direction vector of that axis is an eigenvector, with its corresponding eigenvalue being 1 (because it is not stretched during rotation). This understanding is much more intuitive than a 3×3 matrix.\nCalculation Method # Formula: $A\\vec{v} = \\lambda\\vec{v}$\n$A$: transformation matrix $\\vec{v}$: eigenvector $\\lambda$: eigenvalue To solve, we transform the formula: $A\\vec{v} - \\lambda\\vec{v} = 0$ $A\\vec{v} - \\lambda I\\vec{v} = 0$ $(A - \\lambda I)\\vec{v} = 0$\nThis result shows that the eigenvector $\\vec{v}$ is compressed to the zero vector after transformation by $(A - \\lambda I)$. This means the transformation $(A - \\lambda I)$ is a dimension-reducing transformation, so its determinant must be zero. $\\det(A - \\lambda I) = 0$ Example # For matrix $A = \\begin{bmatrix} 3 \u0026amp; 1 \\ 0 \u0026amp; 2 \\end{bmatrix}$: $\\det(A - \\lambda I) = \\det \\left( \\begin{bmatrix} 3 \u0026amp; 1 \\ 0 \u0026amp; 2 \\end{bmatrix} - \\lambda \\begin{bmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; 1 \\end{bmatrix} \\right) = \\det \\begin{bmatrix} 3-\\lambda \u0026amp; 1 \\ 0 \u0026amp; 2-\\lambda \\end{bmatrix} = (3-\\lambda)(2-\\lambda) = 0$ Solving gives eigenvalues $\\lambda = 2$ or $\\lambda = 3$.\nWhen $\\lambda = 2$, we solve $(A - 2I)\\vec{v}$ = $\\begin{bmatrix} 1 \u0026amp; 1 \\ 0 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix} x \\ y \\end{bmatrix} = \\begin{bmatrix} 0 \\ 0 \\end{bmatrix}$ All solutions lie in the space spanned by vector $\\begin{bmatrix} -1 \\ 1 \\end{bmatrix}$.\nWhen $\\lambda = 3$, the matrix $\\begin{bmatrix} 3 \u0026amp; 1 \\ 0 \u0026amp; 2 \\end{bmatrix}$ will stretch the corresponding eigenvector to 3 times its original size.\nProperties and Applications # Transformations without Eigenvectors # A counterclockwise rotation by 90° in 2D space has no real eigenvectors.\nTransformations with Multiple Eigenvectors # Scaling matrices\nDiagonal Matrices # If the basis vectors of a transformation are all eigenvectors, then the matrix describing this transformation is a diagonal matrix, and the diagonal elements of the matrix are the eigenvalues corresponding to these basis vectors.\nSimplifying High-Power Calculations # Power calculations for diagonal matrices are very simple, just calculate the corresponding powers of the diagonal elements (eigenvalues). When we need to calculate a non-diagonal matrix $M$ multiple times, if it has sufficient eigenvectors, we can:\nUse the eigenvectors as new basis vectors to form the basis transformation matrix $A$. Convert $M$ to a diagonal matrix $D$ through $A^{-1}MA$. Calculate $D^n$. Convert the result back to the original coordinate system through $A D^n A^{-1}$, i.e., $M^n = A D^n A^{-1}$. 11-Abstract Vector Spaces # Vectors and functions have commonalities.\nFunctions as Vectors # Linear Properties # Many operations on functions (such as differentiation) are linear, satisfying:\nAdditivity: $L(f + g) = L(f) + L(g)$\nProportionality: $L(cf) = cL(f)$\nVector representation of polynomials: We can view polynomials as infinite-dimensional vectors.\nTake a set of basis functions $b_0(x)=1, b_1(x)=x, b_2(x)=x^2, \\dots$ as an example. $1x^2 + 3x + 5 \\cdot 1$ can be viewed as vector $\\begin{bmatrix} 5, 3, 1, 0, \\dots \\end{bmatrix}^T$. $4x^7 - 5x^2$ can be viewed as vector $\\begin{bmatrix} 0, 0, -5, 0, 0, 0, 0, 4, \\dots \\end{bmatrix}^T$. Matrix representation of differentiation: The differentiation transformation can also be described by a matrix.\n$\\frac{d}{dx}(a_n x^n + \\dots + a_1 x + a_0) = n a_n x^{n-1} + \\dots + a_1$ This transformation acting on the vector corresponding to the polynomial is like a matrix acting on a vector. Eight Axioms # This is because both vector calculations and function calculations conform to these eight axioms:\nVector addition associativity: $U + (V + W) = (U + V) + W$ Vector addition commutativity: $V + W = W + V$ Additive identity exists: There exists a zero vector $0$ such that $0 + V = V$ Additive inverse exists: For any vector $V$, there exists $-V$ such that $V + (-V) = 0$ Scalar multiplication compatible with field multiplication: $a(bV) = (ab)V$ Scalar multiplication identity exists: $1V = V$ Scalar multiplication distributes over vector addition: $a(V + W) = aV + aW$ Scalar multiplication distributes over field addition: $(a + b)V = aV + bV$ ","date":"25 May 2025","externalUrl":null,"permalink":"/notes/foundation/linearalgebra/essenceoflinearalgebra_part4/","section":"Notes","summary":"","title":"EssenceOfLinearAlgebra - 04","type":"notes"},{"content":" 03-Matrices and Linear Transformations # Linear Transformations # You can think of it as a type of function, meaning there are outputs and inputs. You input a vector and output a vector. The intermediate calculations, the transformation process, reflect the motion laws of vectors.\nCharacteristics # Lines remain lines after transformation (including diagonals formed by vectors) Origin position remains unchanged Grid lines remain parallel and equidistant Linear transformation\nNon-linear transformation\nExample # We can describe linear transformations through numerical values\nOriginal basis vectors: $\\hat{i} = \\begin{bmatrix} 1 \\ 0 \\end{bmatrix}$, $\\hat{j} = \\begin{bmatrix} 0 \\ 1 \\end{bmatrix}$\nWhen considering vector $\\vec{v}$ with coordinates (-1, 2): $ \\vec{v} = -1 \\hat{i} + 2 \\hat{j} = -1 \\begin{bmatrix} 1 \\ 0 \\end{bmatrix} + 2 \\begin{bmatrix} 0 \\ 1 \\end{bmatrix} = \\begin{bmatrix} -1 \\ 2 \\end{bmatrix} $\nIf we apply some transformation, the basis vectors will move together with vector $\\vec{v}$\nTransformed basis vectors: $\\hat{i}\u0026rsquo; = \\begin{bmatrix} 1 \\ -2 \\end{bmatrix}$, $\\hat{j}\u0026rsquo; = \\begin{bmatrix} 3 \\ 0 \\end{bmatrix}$\n$ \\vec{v}\u0026rsquo; = -1 \\hat{i}\u0026rsquo; + 2 \\hat{j}\u0026rsquo; = -1 \\begin{bmatrix} 1 \\ -2 \\end{bmatrix} + 2 \\begin{bmatrix} 3 \\ 0 \\end{bmatrix} = \\begin{bmatrix} -1 \\cdot 1 + 2 \\cdot 3 \\ -1 \\cdot (-2) + 2 \\cdot 0 \\end{bmatrix} = \\begin{bmatrix} 5 \\ 2 \\end{bmatrix} $\nFrom this, we can conclude that we can infer the landing point of $\\vec{v}$ after transformation through $\\hat{i}\u0026rsquo;$ and $\\hat{j}'$\nA 2D linear transformation is completely determined by just four numbers, which are the coordinates of the transformed $\\hat{i}\u0026rsquo;$ and $\\hat{j}\u0026rsquo;$. Usually, we package these coordinates in a 2×2 grid, called a 2×2 matrix. By just inputting a vector $\\begin{bmatrix} -1 \\ 2 \\end{bmatrix}$, we can obtain the transformed vector $\\begin{bmatrix} 5 \\ 2 \\end{bmatrix}$ through $\\begin{bmatrix} 1 \u0026amp; 3 \\ -2 \u0026amp; 0 \\end{bmatrix}$\nFormula # $$ \\begin{bmatrix} a \u0026amp; b \\ c \u0026amp; d \\end{bmatrix} \\begin{bmatrix} x \\ y \\end{bmatrix} = x \\begin{bmatrix} a \\ c \\end{bmatrix} + y \\begin{bmatrix} b \\ d \\end{bmatrix} = \\begin{bmatrix} ax + by \\ cx + dy \\end{bmatrix} $$\nCommon Matrices # Counterclockwise rotation 90° $\\begin{bmatrix} 0 \u0026amp; -1 \\ 1 \u0026amp; 0 \\end{bmatrix}$\nShear $\\begin{bmatrix} 1 \u0026amp; 1 \\ 0 \u0026amp; 1 \\end{bmatrix}$\nColumn linearly dependent $\\begin{bmatrix} 2 \u0026amp; -2 \\ 1 \u0026amp; -1 \\end{bmatrix}$ (meaning one vector is a multiple of the other)\n04-Matrix Multiplication and Linear Transformation Composition # Composition of Transformations # Core definition: An ordered combination of multiple linear transformations, recording the transformation process by tracking the final positions of basis vectors\nCalculation example: First rotate (matrix R = $\\begin{bmatrix} 0 \u0026amp; -1 \\ 1 \u0026amp; 0 \\end{bmatrix}$) then shear (matrix S = $\\begin{bmatrix} 1 \u0026amp; 1 \\ 0 \u0026amp; 1 \\end{bmatrix}$), composite matrix S·R = $\\begin{bmatrix} 1 \u0026amp; -1 \\ 1 \u0026amp; 0 \\end{bmatrix}$\nOrder rule: Matrix multiplication is read from right to left (like f(g(x))), derived from the logical order of function composition\nMatrix Multiplication Calculation # Let $M_1$ = $\\begin{bmatrix} 1 \u0026amp; -2 \\ 1 \u0026amp; 0 \\end{bmatrix}$, $M_2$ = $\\begin{bmatrix} 0 \u0026amp; 2 \\ 1 \u0026amp; 0 \\end{bmatrix}$\nPath of basis vector $\\hat{i}\u0026rsquo;$: $M_1$\u0026rsquo;s $\\hat{i}\u0026rsquo;$ transformed by $M_2$:\n1 × $\\begin{bmatrix} 0 \\ 1 \\end{bmatrix}$ + 1 × $\\begin{bmatrix} 2 \\ 0 \\end{bmatrix}$ = $\\begin{bmatrix} 0 \u0026amp; 2 \\ 1 \u0026amp; 0 \\end{bmatrix}$ = $\\begin{bmatrix} 0 + 2 \\ 1 + 0 \\end{bmatrix}$ = $\\begin{bmatrix} 2 \\ 1 \\end{bmatrix}$\nPath of basis vector $\\hat{j}\u0026rsquo;$: $M_1$\u0026rsquo;s $\\hat{j}\u0026rsquo;$ transformed by $M_2$:\n(-2) × $\\begin{bmatrix} 0 \\ 1 \\end{bmatrix}$ + 0 × $\\begin{bmatrix} 2 \\ 0 \\end{bmatrix}$ = $\\begin{bmatrix} 0 \u0026amp; 0 \\ -2 \u0026amp; 0 \\end{bmatrix}$ = $\\begin{bmatrix} 0 + 0 \\ -2 + 0 \\end{bmatrix}$ = $\\begin{bmatrix} 0 \\ -2 \\end{bmatrix}$\nComposite matrix: $\\begin{bmatrix} 2 \u0026amp; 0 \\ 1 \u0026amp; -2 \\end{bmatrix}$\nGeneral Formula # $M_2 = \\begin{bmatrix} a \u0026amp; b \\ c \u0026amp; d \\end{bmatrix}, M_1 = \\begin{bmatrix} e \u0026amp; f \\ g \u0026amp; h \\end{bmatrix}$\n$M_2M_1 = \\begin{bmatrix} ae + bg \u0026amp; af + bh \\ ce + dg \u0026amp; cf + dh \\end{bmatrix}$\nOperation Properties # Does not satisfy commutativity: $M_1M_2 ≠ M_2M_1$ Satisfies associativity: (AB)C = A(BC) Extension in 3D Space # Additional basis vector: Reference the basis vector $\\hat{k}\u0026rsquo;$ in the Z-axis direction, coordinates $\\begin{bmatrix} 0 \\ 0 \\ 1 \\end{bmatrix}$\nLinear transformation is determined by the positions of $\\hat{i}\u0026rsquo;, \\hat{j}\u0026rsquo;, \\hat{k}\u0026rsquo;$ after transformation\nVector transformation calculation rules are the same as in 2D\n$M_1 \\vec{v} = x \\begin{bmatrix} a \\ b \\ c \\end{bmatrix} + y \\begin{bmatrix} d \\ e \\ f \\end{bmatrix} + z\\begin{bmatrix} g \\ h \\ i \\end{bmatrix}$\n05-Determinant # Geometric Essence of Determinant # The determinant describes the scaling ratio of volume or area by linear transformation\nThe sign represents whether the space is inverted\nRight-Hand Rule # We usually use the right-hand rule, where the ring finger pointing forward represents $\\hat{i}\u0026rsquo;$, the middle finger pointing sideways represents $\\hat{j}\u0026rsquo;$, and the thumb pointing upward represents $\\hat{k}\u0026rsquo;$. If we use the left hand to represent this space, it means the space orientation is flipped, and the determinant is negative\nSpecial Cases # Determinant = 0: Space is compressed to lower dimensions\nDeterminant = 1: Volume remains unchanged\nFormula # $$ \\det \\begin{bmatrix} a \u0026amp; b \\ c \u0026amp; d \\end{bmatrix} = ad - bc $$\nad: Represents the area of the rectangle formed by scaled basis vectors\nbc: Represents how much the parallelogram is stretched or compressed along the diagonal direction\nExample # $$ \\det(M_1M_2) = \\det(M_1) \\cdot \\det(M_2) $$\nAssuming we transform a unit square, M1 scales by 2 times, M2 scales by 4 times,\n$\\det(M_1M_2)$ can be understood as first scaling by 4 times then scaling by 2 times, i.e., 1 × (4 × 2),\n$\\det(M_1) \\cdot \\det(M_2)$ can be understood as 1 × 2 × 4,\nThe two are equal\n06-Inverse Matrix, Column Space, and Null Space # System of Linear Equations # A system of linear equations refers to a collection containing several unknowns and related equations. Each equation has unknowns with only constant coefficients, and these unknowns only undergo addition operations. Such systems can be uniformly represented as vector equations:\nExample:\n$$ \\begin{cases} 2x + 5y + 3z = -3 \\ 4x + 0y + 8z = 0 \\ 1x + 3y + 0z = 2 \\end{cases} $$\nCan be written in matrix form:\n$$ \\begin{bmatrix} 2 \u0026amp; 5 \u0026amp; 3 \\ 4 \u0026amp; 0 \u0026amp; 8 \\ 1 \u0026amp; 3 \u0026amp; 0 \\end{bmatrix} \\begin{bmatrix}x \\ y \\ z\\end{bmatrix} = \\begin{bmatrix}-3 \\ 0 \\ 2\\end{bmatrix} $$\nWhere the left matrix is $A$, the unknown vector is $X$, the right side is $V$, i.e., $AX = V$.\nConcept of Inverse Matrix # The inverse matrix $A^{-1}$ can be understood as a tool to \u0026ldquo;restore\u0026rdquo; transformations. If $AX = V$, then $X = A^{-1}V$, meaning we can solve for the original unknowns through the inverse matrix. The inverse matrix only exists when $A$ is a square matrix and $\\det(A) \\neq 0$. If the matrix \u0026ldquo;compresses\u0026rdquo; the space (reduces dimensions), the inverse matrix doesn\u0026rsquo;t exist. Special case: If after dimension reduction, a certain vector happens to fall on the target space, this vector can be restored by inverse transformation. Terminology Explanation # Rank: The dimension after space transformation. If transformed to 1D, rank is 1; if 2D, rank is 2; and so on. Column Space: The set of all possible output vectors, i.e., the space formed by $Av$, spanned by the column vectors of matrix $A$. Difference between Span and Column Space: Span refers to all possible results of linear combinations of any vector set, while column space is the set of all possible results after multiplying a matrix with any vector. Span can be applied to any vector set, while column space is directly related to the existence of solutions to the linear system $Ax = v$. Full Rank: Rank equals the number of columns. Zero Vector: Vector with coordinates (0, 0). When full rank, the zero vector is the origin; when not full rank, multiple vectors become zero vectors. Null Space/Kernel: The set of all vectors that land at the origin after transformation. 6-Supplementary Note: Non-Square Matrices # Non-square matrices are matrices where the number of rows and columns are not equal ($m \\neq n$). Examples: $\\begin{bmatrix}2 \u0026amp; -1 \u0026amp; 2 \\ 0 \u0026amp; 1 \u0026amp; 1\\end{bmatrix}$: 2 rows 3 columns, represents mapping from 3D space to 2D space (e.g., points in 3D space are projected onto a plane). $\\begin{bmatrix}3 \u0026amp; 1 \\ 1 \u0026amp; 5 \\ 4 \u0026amp; 9\\end{bmatrix}$: 3 rows 2 columns, represents mapping from 2D space to 3D space (e.g., points on a 2D plane are stretched into 3D space). $[1, 2]$: 1 row 2 columns, represents mapping from 2D space to 1D space (e.g., points in 2D space are compressed onto a line). ","date":"24 May 2025","externalUrl":null,"permalink":"/notes/foundation/linearalgebra/essenceoflinearalgebra_part2/","section":"Notes","summary":"","title":"EssenceOfLinearAlgebra - 02","type":"notes"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]